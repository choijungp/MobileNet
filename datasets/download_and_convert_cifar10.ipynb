{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "r\"\"\"Downloads and converts cifar10 data to TFRecords of TF-Example protos.\n",
        "This module downloads the cifar10 data, uncompresses it, reads the files\n",
        "that make up the cifar10 data and creates two TFRecord datasets: one for train\n",
        "and one for test. Each TFRecord dataset is comprised of a set of TF-Example\n",
        "protocol buffers, each of which contain a single image and label.\n",
        "The script should take several minutes to run.\n",
        "\"\"\"\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import cPickle\n",
        "import os\n",
        "import sys\n",
        "import tarfile\n",
        "\n",
        "import numpy as np\n",
        "from six.moves import urllib\n",
        "import tensorflow as tf\n",
        "\n",
        "from datasets import dataset_utils\n",
        "\n",
        "# The URL where the CIFAR data can be downloaded.\n",
        "_DATA_URL = 'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz'\n",
        "\n",
        "# The number of training files.\n",
        "_NUM_TRAIN_FILES = 5\n",
        "\n",
        "# The height and width of each image.\n",
        "_IMAGE_SIZE = 32\n",
        "\n",
        "# The names of the classes.\n",
        "_CLASS_NAMES = [\n",
        "    'airplane',\n",
        "    'automobile',\n",
        "    'bird',\n",
        "    'cat',\n",
        "    'deer',\n",
        "    'dog',\n",
        "    'frog',\n",
        "    'horse',\n",
        "    'ship',\n",
        "    'truck',\n",
        "]\n",
        "\n",
        "\n",
        "def _add_to_tfrecord(filename, tfrecord_writer, offset=0):\n",
        "  \"\"\"Loads data from the cifar10 pickle files and writes files to a TFRecord.\n",
        "  Args:\n",
        "    filename: The filename of the cifar10 pickle file.\n",
        "    tfrecord_writer: The TFRecord writer to use for writing.\n",
        "    offset: An offset into the absolute number of images previously written.\n",
        "  Returns:\n",
        "    The new offset.\n",
        "  \"\"\"\n",
        "  with tf.gfile.Open(filename, 'r') as f:\n",
        "    data = cPickle.load(f)\n",
        "\n",
        "  images = data['data']\n",
        "  num_images = images.shape[0]\n",
        "\n",
        "  images = images.reshape((num_images, 3, 32, 32))\n",
        "  labels = data['labels']\n",
        "\n",
        "  with tf.Graph().as_default():\n",
        "    image_placeholder = tf.placeholder(dtype=tf.uint8)\n",
        "    encoded_image = tf.image.encode_png(image_placeholder)\n",
        "\n",
        "    with tf.Session('') as sess:\n",
        "\n",
        "      for j in range(num_images):\n",
        "        sys.stdout.write('\\r>> Reading file [%s] image %d/%d' % (\n",
        "            filename, offset + j + 1, offset + num_images))\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        image = np.squeeze(images[j]).transpose((1, 2, 0))\n",
        "        label = labels[j]\n",
        "\n",
        "        png_string = sess.run(encoded_image,\n",
        "                              feed_dict={image_placeholder: image})\n",
        "\n",
        "        example = dataset_utils.image_to_tfexample(\n",
        "            png_string, 'png', _IMAGE_SIZE, _IMAGE_SIZE, label)\n",
        "        tfrecord_writer.write(example.SerializeToString())\n",
        "\n",
        "  return offset + num_images\n",
        "\n",
        "\n",
        "def _get_output_filename(dataset_dir, split_name):\n",
        "  \"\"\"Creates the output filename.\n",
        "  Args:\n",
        "    dataset_dir: The dataset directory where the dataset is stored.\n",
        "    split_name: The name of the train/test split.\n",
        "  Returns:\n",
        "    An absolute file path.\n",
        "  \"\"\"\n",
        "  return '%s/cifar10_%s.tfrecord' % (dataset_dir, split_name)\n",
        "\n",
        "\n",
        "def _download_and_uncompress_dataset(dataset_dir):\n",
        "  \"\"\"Downloads cifar10 and uncompresses it locally.\n",
        "  Args:\n",
        "    dataset_dir: The directory where the temporary files are stored.\n",
        "  \"\"\"\n",
        "  filename = _DATA_URL.split('/')[-1]\n",
        "  filepath = os.path.join(dataset_dir, filename)\n",
        "\n",
        "  if not os.path.exists(filepath):\n",
        "    def _progress(count, block_size, total_size):\n",
        "      sys.stdout.write('\\r>> Downloading %s %.1f%%' % (\n",
        "          filename, float(count * block_size) / float(total_size) * 100.0))\n",
        "      sys.stdout.flush()\n",
        "    filepath, _ = urllib.request.urlretrieve(_DATA_URL, filepath, _progress)\n",
        "    print()\n",
        "    statinfo = os.stat(filepath)\n",
        "    print('Successfully downloaded', filename, statinfo.st_size, 'bytes.')\n",
        "    tarfile.open(filepath, 'r:gz').extractall(dataset_dir)\n",
        "\n",
        "\n",
        "def _clean_up_temporary_files(dataset_dir):\n",
        "  \"\"\"Removes temporary files used to create the dataset.\n",
        "  Args:\n",
        "    dataset_dir: The directory where the temporary files are stored.\n",
        "  \"\"\"\n",
        "  filename = _DATA_URL.split('/')[-1]\n",
        "  filepath = os.path.join(dataset_dir, filename)\n",
        "  tf.gfile.Remove(filepath)\n",
        "\n",
        "  tmp_dir = os.path.join(dataset_dir, 'cifar-10-batches-py')\n",
        "  tf.gfile.DeleteRecursively(tmp_dir)\n",
        "\n",
        "\n",
        "def run(dataset_dir):\n",
        "  \"\"\"Runs the download and conversion operation.\n",
        "  Args:\n",
        "    dataset_dir: The dataset directory where the dataset is stored.\n",
        "  \"\"\"\n",
        "  if not tf.gfile.Exists(dataset_dir):\n",
        "    tf.gfile.MakeDirs(dataset_dir)\n",
        "\n",
        "  training_filename = _get_output_filename(dataset_dir, 'train')\n",
        "  testing_filename = _get_output_filename(dataset_dir, 'test')\n",
        "\n",
        "  if tf.gfile.Exists(training_filename) and tf.gfile.Exists(testing_filename):\n",
        "    print('Dataset files already exist. Exiting without re-creating them.')\n",
        "    return\n",
        "\n",
        "  dataset_utils.download_and_uncompress_tarball(_DATA_URL, dataset_dir)\n",
        "\n",
        "  # First, process the training data:\n",
        "  with tf.python_io.TFRecordWriter(training_filename) as tfrecord_writer:\n",
        "    offset = 0\n",
        "    for i in range(_NUM_TRAIN_FILES):\n",
        "      filename = os.path.join(dataset_dir,\n",
        "                              'cifar-10-batches-py',\n",
        "                              'data_batch_%d' % (i + 1))  # 1-indexed.\n",
        "      offset = _add_to_tfrecord(filename, tfrecord_writer, offset)\n",
        "\n",
        "  # Next, process the testing data:\n",
        "  with tf.python_io.TFRecordWriter(testing_filename) as tfrecord_writer:\n",
        "    filename = os.path.join(dataset_dir,\n",
        "                            'cifar-10-batches-py',\n",
        "                            'test_batch')\n",
        "    _add_to_tfrecord(filename, tfrecord_writer)\n",
        "\n",
        "  # Finally, write the labels file:\n",
        "  labels_to_class_names = dict(zip(range(len(_CLASS_NAMES)), _CLASS_NAMES))\n",
        "  dataset_utils.write_label_file(labels_to_class_names, dataset_dir)\n",
        "\n",
        "  _clean_up_temporary_files(dataset_dir)\n",
        "  print('\\nFinished converting the Cifar10 dataset!')"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}