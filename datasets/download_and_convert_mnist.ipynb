{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "r\"\"\"Downloads and converts MNIST data to TFRecords of TF-Example protos.\n",
        "This module downloads the MNIST data, uncompresses it, reads the files\n",
        "that make up the MNIST data and creates two TFRecord datasets: one for train\n",
        "and one for test. Each TFRecord dataset is comprised of a set of TF-Example\n",
        "protocol buffers, each of which contain a single image and label.\n",
        "The script should take about a minute to run.\n",
        "\"\"\"\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import gzip\n",
        "import os\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "from six.moves import urllib\n",
        "import tensorflow as tf\n",
        "\n",
        "from datasets import dataset_utils\n",
        "\n",
        "# The URLs where the MNIST data can be downloaded.\n",
        "_DATA_URL = 'http://yann.lecun.com/exdb/mnist/'\n",
        "_TRAIN_DATA_FILENAME = 'train-images-idx3-ubyte.gz'\n",
        "_TRAIN_LABELS_FILENAME = 'train-labels-idx1-ubyte.gz'\n",
        "_TEST_DATA_FILENAME = 't10k-images-idx3-ubyte.gz'\n",
        "_TEST_LABELS_FILENAME = 't10k-labels-idx1-ubyte.gz'\n",
        "\n",
        "_IMAGE_SIZE = 28\n",
        "_NUM_CHANNELS = 1\n",
        "\n",
        "# The names of the classes.\n",
        "_CLASS_NAMES = [\n",
        "    'zero',\n",
        "    'one',\n",
        "    'two',\n",
        "    'three',\n",
        "    'four',\n",
        "    'five',\n",
        "    'size',\n",
        "    'seven',\n",
        "    'eight',\n",
        "    'nine',\n",
        "]\n",
        "\n",
        "\n",
        "def _extract_images(filename, num_images):\n",
        "  \"\"\"Extract the images into a numpy array.\n",
        "  Args:\n",
        "    filename: The path to an MNIST images file.\n",
        "    num_images: The number of images in the file.\n",
        "  Returns:\n",
        "    A numpy array of shape [number_of_images, height, width, channels].\n",
        "  \"\"\"\n",
        "  print('Extracting images from: ', filename)\n",
        "  with gzip.open(filename) as bytestream:\n",
        "    bytestream.read(16)\n",
        "    buf = bytestream.read(\n",
        "        _IMAGE_SIZE * _IMAGE_SIZE * num_images * _NUM_CHANNELS)\n",
        "    data = np.frombuffer(buf, dtype=np.uint8)\n",
        "    data = data.reshape(num_images, _IMAGE_SIZE, _IMAGE_SIZE, _NUM_CHANNELS)\n",
        "  return data\n",
        "\n",
        "\n",
        "def _extract_labels(filename, num_labels):\n",
        "  \"\"\"Extract the labels into a vector of int64 label IDs.\n",
        "  Args:\n",
        "    filename: The path to an MNIST labels file.\n",
        "    num_labels: The number of labels in the file.\n",
        "  Returns:\n",
        "    A numpy array of shape [number_of_labels]\n",
        "  \"\"\"\n",
        "  print('Extracting labels from: ', filename)\n",
        "  with gzip.open(filename) as bytestream:\n",
        "    bytestream.read(8)\n",
        "    buf = bytestream.read(1 * num_labels)\n",
        "    labels = np.frombuffer(buf, dtype=np.uint8).astype(np.int64)\n",
        "  return labels\n",
        "\n",
        "\n",
        "def _add_to_tfrecord(data_filename, labels_filename, num_images,\n",
        "                     tfrecord_writer):\n",
        "  \"\"\"Loads data from the binary MNIST files and writes files to a TFRecord.\n",
        "  Args:\n",
        "    data_filename: The filename of the MNIST images.\n",
        "    labels_filename: The filename of the MNIST labels.\n",
        "    num_images: The number of images in the dataset.\n",
        "    tfrecord_writer: The TFRecord writer to use for writing.\n",
        "  \"\"\"\n",
        "  images = _extract_images(data_filename, num_images)\n",
        "  labels = _extract_labels(labels_filename, num_images)\n",
        "\n",
        "  shape = (_IMAGE_SIZE, _IMAGE_SIZE, _NUM_CHANNELS)\n",
        "  with tf.Graph().as_default():\n",
        "    image = tf.placeholder(dtype=tf.uint8, shape=shape)\n",
        "    encoded_png = tf.image.encode_png(image)\n",
        "\n",
        "    with tf.Session('') as sess:\n",
        "      for j in range(num_images):\n",
        "        sys.stdout.write('\\r>> Converting image %d/%d' % (j + 1, num_images))\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        png_string = sess.run(encoded_png, feed_dict={image: images[j]})\n",
        "\n",
        "        example = dataset_utils.image_to_tfexample(\n",
        "            png_string, 'png'.encode(), _IMAGE_SIZE, _IMAGE_SIZE, labels[j])\n",
        "        tfrecord_writer.write(example.SerializeToString())\n",
        "\n",
        "\n",
        "def _get_output_filename(dataset_dir, split_name):\n",
        "  \"\"\"Creates the output filename.\n",
        "  Args:\n",
        "    dataset_dir: The directory where the temporary files are stored.\n",
        "    split_name: The name of the train/test split.\n",
        "  Returns:\n",
        "    An absolute file path.\n",
        "  \"\"\"\n",
        "  return '%s/mnist_%s.tfrecord' % (dataset_dir, split_name)\n",
        "\n",
        "\n",
        "def _download_dataset(dataset_dir):\n",
        "  \"\"\"Downloads MNIST locally.\n",
        "  Args:\n",
        "    dataset_dir: The directory where the temporary files are stored.\n",
        "  \"\"\"\n",
        "  for filename in [_TRAIN_DATA_FILENAME,\n",
        "                   _TRAIN_LABELS_FILENAME,\n",
        "                   _TEST_DATA_FILENAME,\n",
        "                   _TEST_LABELS_FILENAME]:\n",
        "    filepath = os.path.join(dataset_dir, filename)\n",
        "\n",
        "    if not os.path.exists(filepath):\n",
        "      print('Downloading file %s...' % filename)\n",
        "      def _progress(count, block_size, total_size):\n",
        "        sys.stdout.write('\\r>> Downloading %.1f%%' % (\n",
        "            float(count * block_size) / float(total_size) * 100.0))\n",
        "        sys.stdout.flush()\n",
        "      filepath, _ = urllib.request.urlretrieve(_DATA_URL + filename,\n",
        "                                               filepath,\n",
        "                                               _progress)\n",
        "      print()\n",
        "      with tf.gfile.GFile(filepath) as f:\n",
        "        size = f.size()\n",
        "      print('Successfully downloaded', filename, size, 'bytes.')\n",
        "\n",
        "\n",
        "def _clean_up_temporary_files(dataset_dir):\n",
        "  \"\"\"Removes temporary files used to create the dataset.\n",
        "  Args:\n",
        "    dataset_dir: The directory where the temporary files are stored.\n",
        "  \"\"\"\n",
        "  for filename in [_TRAIN_DATA_FILENAME,\n",
        "                   _TRAIN_LABELS_FILENAME,\n",
        "                   _TEST_DATA_FILENAME,\n",
        "                   _TEST_LABELS_FILENAME]:\n",
        "    filepath = os.path.join(dataset_dir, filename)\n",
        "    tf.gfile.Remove(filepath)\n",
        "\n",
        "\n",
        "def run(dataset_dir):\n",
        "  \"\"\"Runs the download and conversion operation.\n",
        "  Args:\n",
        "    dataset_dir: The dataset directory where the dataset is stored.\n",
        "  \"\"\"\n",
        "  if not tf.gfile.Exists(dataset_dir):\n",
        "    tf.gfile.MakeDirs(dataset_dir)\n",
        "\n",
        "  training_filename = _get_output_filename(dataset_dir, 'train')\n",
        "  testing_filename = _get_output_filename(dataset_dir, 'test')\n",
        "\n",
        "  if tf.gfile.Exists(training_filename) and tf.gfile.Exists(testing_filename):\n",
        "    print('Dataset files already exist. Exiting without re-creating them.')\n",
        "    return\n",
        "\n",
        "  _download_dataset(dataset_dir)\n",
        "\n",
        "  # First, process the training data:\n",
        "  with tf.python_io.TFRecordWriter(training_filename) as tfrecord_writer:\n",
        "    data_filename = os.path.join(dataset_dir, _TRAIN_DATA_FILENAME)\n",
        "    labels_filename = os.path.join(dataset_dir, _TRAIN_LABELS_FILENAME)\n",
        "    _add_to_tfrecord(data_filename, labels_filename, 60000, tfrecord_writer)\n",
        "\n",
        "  # Next, process the testing data:\n",
        "  with tf.python_io.TFRecordWriter(testing_filename) as tfrecord_writer:\n",
        "    data_filename = os.path.join(dataset_dir, _TEST_DATA_FILENAME)\n",
        "    labels_filename = os.path.join(dataset_dir, _TEST_LABELS_FILENAME)\n",
        "    _add_to_tfrecord(data_filename, labels_filename, 10000, tfrecord_writer)\n",
        "\n",
        "  # Finally, write the labels file:\n",
        "  labels_to_class_names = dict(zip(range(len(_CLASS_NAMES)), _CLASS_NAMES))\n",
        "  dataset_utils.write_label_file(labels_to_class_names, dataset_dir)\n",
        "\n",
        "  _clean_up_temporary_files(dataset_dir)\n",
        "  print('\\nFinished converting the MNIST dataset!')"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}