{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\"\"\"Contains the definition of the Inception Resnet V2 architecture.\n",
        "As described in http://arxiv.org/abs/1602.07261.\n",
        "  Inception-v4, Inception-ResNet and the Impact of Residual Connections\n",
        "    on Learning\n",
        "  Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi\n",
        "\"\"\"\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "slim = tf.contrib.slim\n",
        "\n",
        "\n",
        "def block35(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n",
        "  \"\"\"Builds the 35x35 resnet block.\"\"\"\n",
        "  with tf.variable_scope(scope, 'Block35', [net], reuse=reuse):\n",
        "    with tf.variable_scope('Branch_0'):\n",
        "      tower_conv = slim.conv2d(net, 32, 1, scope='Conv2d_1x1')\n",
        "    with tf.variable_scope('Branch_1'):\n",
        "      tower_conv1_0 = slim.conv2d(net, 32, 1, scope='Conv2d_0a_1x1')\n",
        "      tower_conv1_1 = slim.conv2d(tower_conv1_0, 32, 3, scope='Conv2d_0b_3x3')\n",
        "    with tf.variable_scope('Branch_2'):\n",
        "      tower_conv2_0 = slim.conv2d(net, 32, 1, scope='Conv2d_0a_1x1')\n",
        "      tower_conv2_1 = slim.conv2d(tower_conv2_0, 48, 3, scope='Conv2d_0b_3x3')\n",
        "      tower_conv2_2 = slim.conv2d(tower_conv2_1, 64, 3, scope='Conv2d_0c_3x3')\n",
        "    mixed = tf.concat(axis=3, values=[tower_conv, tower_conv1_1, tower_conv2_2])\n",
        "    up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n",
        "                     activation_fn=None, scope='Conv2d_1x1')\n",
        "    net += scale * up\n",
        "    if activation_fn:\n",
        "      net = activation_fn(net)\n",
        "  return net\n",
        "\n",
        "\n",
        "def block17(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n",
        "  \"\"\"Builds the 17x17 resnet block.\"\"\"\n",
        "  with tf.variable_scope(scope, 'Block17', [net], reuse=reuse):\n",
        "    with tf.variable_scope('Branch_0'):\n",
        "      tower_conv = slim.conv2d(net, 192, 1, scope='Conv2d_1x1')\n",
        "    with tf.variable_scope('Branch_1'):\n",
        "      tower_conv1_0 = slim.conv2d(net, 128, 1, scope='Conv2d_0a_1x1')\n",
        "      tower_conv1_1 = slim.conv2d(tower_conv1_0, 160, [1, 7],\n",
        "                                  scope='Conv2d_0b_1x7')\n",
        "      tower_conv1_2 = slim.conv2d(tower_conv1_1, 192, [7, 1],\n",
        "                                  scope='Conv2d_0c_7x1')\n",
        "    mixed = tf.concat(axis=3, values=[tower_conv, tower_conv1_2])\n",
        "    up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n",
        "                     activation_fn=None, scope='Conv2d_1x1')\n",
        "    net += scale * up\n",
        "    if activation_fn:\n",
        "      net = activation_fn(net)\n",
        "  return net\n",
        "\n",
        "\n",
        "def block8(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n",
        "  \"\"\"Builds the 8x8 resnet block.\"\"\"\n",
        "  with tf.variable_scope(scope, 'Block8', [net], reuse=reuse):\n",
        "    with tf.variable_scope('Branch_0'):\n",
        "      tower_conv = slim.conv2d(net, 192, 1, scope='Conv2d_1x1')\n",
        "    with tf.variable_scope('Branch_1'):\n",
        "      tower_conv1_0 = slim.conv2d(net, 192, 1, scope='Conv2d_0a_1x1')\n",
        "      tower_conv1_1 = slim.conv2d(tower_conv1_0, 224, [1, 3],\n",
        "                                  scope='Conv2d_0b_1x3')\n",
        "      tower_conv1_2 = slim.conv2d(tower_conv1_1, 256, [3, 1],\n",
        "                                  scope='Conv2d_0c_3x1')\n",
        "    mixed = tf.concat(axis=3, values=[tower_conv, tower_conv1_2])\n",
        "    up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n",
        "                     activation_fn=None, scope='Conv2d_1x1')\n",
        "    net += scale * up\n",
        "    if activation_fn:\n",
        "      net = activation_fn(net)\n",
        "  return net\n",
        "\n",
        "\n",
        "def inception_resnet_v2(inputs, num_classes=1001, is_training=True,\n",
        "                        dropout_keep_prob=0.8,\n",
        "                        reuse=None,\n",
        "                        scope='InceptionResnetV2'):\n",
        "  \"\"\"Creates the Inception Resnet V2 model.\n",
        "  Args:\n",
        "    inputs: a 4-D tensor of size [batch_size, height, width, 3].\n",
        "    num_classes: number of predicted classes.\n",
        "    is_training: whether is training or not.\n",
        "    dropout_keep_prob: float, the fraction to keep before final layer.\n",
        "    reuse: whether or not the network and its variables should be reused. To be\n",
        "      able to reuse 'scope' must be given.\n",
        "    scope: Optional variable_scope.\n",
        "  Returns:\n",
        "    logits: the logits outputs of the model.\n",
        "    end_points: the set of end_points from the inception model.\n",
        "  \"\"\"\n",
        "  end_points = {}\n",
        "\n",
        "  with tf.variable_scope(scope, 'InceptionResnetV2', [inputs], reuse=reuse):\n",
        "    with slim.arg_scope([slim.batch_norm, slim.dropout],\n",
        "                        is_training=is_training):\n",
        "      with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n",
        "                          stride=1, padding='SAME'):\n",
        "\n",
        "        # 149 x 149 x 32\n",
        "        net = slim.conv2d(inputs, 32, 3, stride=2, padding='VALID',\n",
        "                          scope='Conv2d_1a_3x3')\n",
        "        end_points['Conv2d_1a_3x3'] = net\n",
        "        # 147 x 147 x 32\n",
        "        net = slim.conv2d(net, 32, 3, padding='VALID',\n",
        "                          scope='Conv2d_2a_3x3')\n",
        "        end_points['Conv2d_2a_3x3'] = net\n",
        "        # 147 x 147 x 64\n",
        "        net = slim.conv2d(net, 64, 3, scope='Conv2d_2b_3x3')\n",
        "        end_points['Conv2d_2b_3x3'] = net\n",
        "        # 73 x 73 x 64\n",
        "        net = slim.max_pool2d(net, 3, stride=2, padding='VALID',\n",
        "                              scope='MaxPool_3a_3x3')\n",
        "        end_points['MaxPool_3a_3x3'] = net\n",
        "        # 73 x 73 x 80\n",
        "        net = slim.conv2d(net, 80, 1, padding='VALID',\n",
        "                          scope='Conv2d_3b_1x1')\n",
        "        end_points['Conv2d_3b_1x1'] = net\n",
        "        # 71 x 71 x 192\n",
        "        net = slim.conv2d(net, 192, 3, padding='VALID',\n",
        "                          scope='Conv2d_4a_3x3')\n",
        "        end_points['Conv2d_4a_3x3'] = net\n",
        "        # 35 x 35 x 192\n",
        "        net = slim.max_pool2d(net, 3, stride=2, padding='VALID',\n",
        "                              scope='MaxPool_5a_3x3')\n",
        "        end_points['MaxPool_5a_3x3'] = net\n",
        "\n",
        "        # 35 x 35 x 320\n",
        "        with tf.variable_scope('Mixed_5b'):\n",
        "          with tf.variable_scope('Branch_0'):\n",
        "            tower_conv = slim.conv2d(net, 96, 1, scope='Conv2d_1x1')\n",
        "          with tf.variable_scope('Branch_1'):\n",
        "            tower_conv1_0 = slim.conv2d(net, 48, 1, scope='Conv2d_0a_1x1')\n",
        "            tower_conv1_1 = slim.conv2d(tower_conv1_0, 64, 5,\n",
        "                                        scope='Conv2d_0b_5x5')\n",
        "          with tf.variable_scope('Branch_2'):\n",
        "            tower_conv2_0 = slim.conv2d(net, 64, 1, scope='Conv2d_0a_1x1')\n",
        "            tower_conv2_1 = slim.conv2d(tower_conv2_0, 96, 3,\n",
        "                                        scope='Conv2d_0b_3x3')\n",
        "            tower_conv2_2 = slim.conv2d(tower_conv2_1, 96, 3,\n",
        "                                        scope='Conv2d_0c_3x3')\n",
        "          with tf.variable_scope('Branch_3'):\n",
        "            tower_pool = slim.avg_pool2d(net, 3, stride=1, padding='SAME',\n",
        "                                         scope='AvgPool_0a_3x3')\n",
        "            tower_pool_1 = slim.conv2d(tower_pool, 64, 1,\n",
        "                                       scope='Conv2d_0b_1x1')\n",
        "          net = tf.concat(axis=3, values=[tower_conv, tower_conv1_1,\n",
        "                              tower_conv2_2, tower_pool_1])\n",
        "\n",
        "        end_points['Mixed_5b'] = net\n",
        "        net = slim.repeat(net, 10, block35, scale=0.17)\n",
        "\n",
        "        # 17 x 17 x 1088\n",
        "        with tf.variable_scope('Mixed_6a'):\n",
        "          with tf.variable_scope('Branch_0'):\n",
        "            tower_conv = slim.conv2d(net, 384, 3, stride=2, padding='VALID',\n",
        "                                     scope='Conv2d_1a_3x3')\n",
        "          with tf.variable_scope('Branch_1'):\n",
        "            tower_conv1_0 = slim.conv2d(net, 256, 1, scope='Conv2d_0a_1x1')\n",
        "            tower_conv1_1 = slim.conv2d(tower_conv1_0, 256, 3,\n",
        "                                        scope='Conv2d_0b_3x3')\n",
        "            tower_conv1_2 = slim.conv2d(tower_conv1_1, 384, 3,\n",
        "                                        stride=2, padding='VALID',\n",
        "                                        scope='Conv2d_1a_3x3')\n",
        "          with tf.variable_scope('Branch_2'):\n",
        "            tower_pool = slim.max_pool2d(net, 3, stride=2, padding='VALID',\n",
        "                                         scope='MaxPool_1a_3x3')\n",
        "          net = tf.concat(axis=3, values=[tower_conv, tower_conv1_2, tower_pool])\n",
        "\n",
        "        end_points['Mixed_6a'] = net\n",
        "        net = slim.repeat(net, 20, block17, scale=0.10)\n",
        "\n",
        "        # Auxiliary tower\n",
        "        with tf.variable_scope('AuxLogits'):\n",
        "          aux = slim.avg_pool2d(net, 5, stride=3, padding='VALID',\n",
        "                                scope='Conv2d_1a_3x3')\n",
        "          aux = slim.conv2d(aux, 128, 1, scope='Conv2d_1b_1x1')\n",
        "          aux = slim.conv2d(aux, 768, aux.get_shape()[1:3],\n",
        "                            padding='VALID', scope='Conv2d_2a_5x5')\n",
        "          aux = slim.flatten(aux)\n",
        "          aux = slim.fully_connected(aux, num_classes, activation_fn=None,\n",
        "                                     scope='Logits')\n",
        "          end_points['AuxLogits'] = aux\n",
        "\n",
        "        with tf.variable_scope('Mixed_7a'):\n",
        "          with tf.variable_scope('Branch_0'):\n",
        "            tower_conv = slim.conv2d(net, 256, 1, scope='Conv2d_0a_1x1')\n",
        "            tower_conv_1 = slim.conv2d(tower_conv, 384, 3, stride=2,\n",
        "                                       padding='VALID', scope='Conv2d_1a_3x3')\n",
        "          with tf.variable_scope('Branch_1'):\n",
        "            tower_conv1 = slim.conv2d(net, 256, 1, scope='Conv2d_0a_1x1')\n",
        "            tower_conv1_1 = slim.conv2d(tower_conv1, 288, 3, stride=2,\n",
        "                                        padding='VALID', scope='Conv2d_1a_3x3')\n",
        "          with tf.variable_scope('Branch_2'):\n",
        "            tower_conv2 = slim.conv2d(net, 256, 1, scope='Conv2d_0a_1x1')\n",
        "            tower_conv2_1 = slim.conv2d(tower_conv2, 288, 3,\n",
        "                                        scope='Conv2d_0b_3x3')\n",
        "            tower_conv2_2 = slim.conv2d(tower_conv2_1, 320, 3, stride=2,\n",
        "                                        padding='VALID', scope='Conv2d_1a_3x3')\n",
        "          with tf.variable_scope('Branch_3'):\n",
        "            tower_pool = slim.max_pool2d(net, 3, stride=2, padding='VALID',\n",
        "                                         scope='MaxPool_1a_3x3')\n",
        "          net = tf.concat(axis=3, values=[tower_conv_1, tower_conv1_1,\n",
        "                              tower_conv2_2, tower_pool])\n",
        "\n",
        "        end_points['Mixed_7a'] = net\n",
        "\n",
        "        net = slim.repeat(net, 9, block8, scale=0.20)\n",
        "        net = block8(net, activation_fn=None)\n",
        "\n",
        "        net = slim.conv2d(net, 1536, 1, scope='Conv2d_7b_1x1')\n",
        "        end_points['Conv2d_7b_1x1'] = net\n",
        "\n",
        "        with tf.variable_scope('Logits'):\n",
        "          end_points['PrePool'] = net\n",
        "          net = slim.avg_pool2d(net, net.get_shape()[1:3], padding='VALID',\n",
        "                                scope='AvgPool_1a_8x8')\n",
        "          net = slim.flatten(net)\n",
        "\n",
        "          net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n",
        "                             scope='Dropout')\n",
        "\n",
        "          end_points['PreLogitsFlatten'] = net\n",
        "          logits = slim.fully_connected(net, num_classes, activation_fn=None,\n",
        "                                        scope='Logits')\n",
        "          end_points['Logits'] = logits\n",
        "          end_points['Predictions'] = tf.nn.softmax(logits, name='Predictions')\n",
        "\n",
        "    return logits, end_points\n",
        "inception_resnet_v2.default_image_size = 299\n",
        "\n",
        "\n",
        "def inception_resnet_v2_arg_scope(weight_decay=0.00004,\n",
        "                                  batch_norm_decay=0.9997,\n",
        "                                  batch_norm_epsilon=0.001):\n",
        "  \"\"\"Yields the scope with the default parameters for inception_resnet_v2.\n",
        "  Args:\n",
        "    weight_decay: the weight decay for weights variables.\n",
        "    batch_norm_decay: decay for the moving average of batch_norm momentums.\n",
        "    batch_norm_epsilon: small float added to variance to avoid dividing by zero.\n",
        "  Returns:\n",
        "    a arg_scope with the parameters needed for inception_resnet_v2.\n",
        "  \"\"\"\n",
        "  # Set weight_decay for weights in conv2d and fully_connected layers.\n",
        "  with slim.arg_scope([slim.conv2d, slim.fully_connected],\n",
        "                      weights_regularizer=slim.l2_regularizer(weight_decay),\n",
        "                      biases_regularizer=slim.l2_regularizer(weight_decay)):\n",
        "\n",
        "    batch_norm_params = {\n",
        "        'decay': batch_norm_decay,\n",
        "        'epsilon': batch_norm_epsilon,\n",
        "    }\n",
        "    # Set activation_fn and parameters for batch_norm.\n",
        "    with slim.arg_scope([slim.conv2d], activation_fn=tf.nn.relu,\n",
        "                        normalizer_fn=slim.batch_norm,\n",
        "                        normalizer_params=batch_norm_params) as scope:\n",
        "      return scope"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}