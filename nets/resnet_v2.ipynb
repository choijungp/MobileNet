{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\"\"\"Contains definitions for the preactivation form of Residual Networks.\n",
        "Residual networks (ResNets) were originally proposed in:\n",
        "[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n",
        "    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n",
        "The full preactivation 'v2' ResNet variant implemented in this module was\n",
        "introduced by:\n",
        "[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n",
        "    Identity Mappings in Deep Residual Networks. arXiv: 1603.05027\n",
        "The key difference of the full preactivation 'v2' variant compared to the\n",
        "'v1' variant in [1] is the use of batch normalization before every weight layer.\n",
        "Another difference is that 'v2' ResNets do not include an activation function in\n",
        "the main pathway. Also see [2; Fig. 4e].\n",
        "Typical use:\n",
        "   from tensorflow.contrib.slim.nets import resnet_v2\n",
        "ResNet-101 for image classification into 1000 classes:\n",
        "   # inputs has shape [batch, 224, 224, 3]\n",
        "   with slim.arg_scope(resnet_v2.resnet_arg_scope()):\n",
        "      net, end_points = resnet_v2.resnet_v2_101(inputs, 1000, is_training=False)\n",
        "ResNet-101 for semantic segmentation into 21 classes:\n",
        "   # inputs has shape [batch, 513, 513, 3]\n",
        "   with slim.arg_scope(resnet_v2.resnet_arg_scope(is_training)):\n",
        "      net, end_points = resnet_v2.resnet_v2_101(inputs,\n",
        "                                                21,\n",
        "                                                is_training=False,\n",
        "                                                global_pool=False,\n",
        "                                                output_stride=16)\n",
        "\"\"\"\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from nets import resnet_utils\n",
        "\n",
        "slim = tf.contrib.slim\n",
        "resnet_arg_scope = resnet_utils.resnet_arg_scope\n",
        "\n",
        "\n",
        "@slim.add_arg_scope\n",
        "def bottleneck(inputs, depth, depth_bottleneck, stride, rate=1,\n",
        "               outputs_collections=None, scope=None):\n",
        "  \"\"\"Bottleneck residual unit variant with BN before convolutions.\n",
        "  This is the full preactivation residual unit variant proposed in [2]. See\n",
        "  Fig. 1(b) of [2] for its definition. Note that we use here the bottleneck\n",
        "  variant which has an extra bottleneck layer.\n",
        "  When putting together two consecutive ResNet blocks that use this unit, one\n",
        "  should use stride = 2 in the last unit of the first block.\n",
        "  Args:\n",
        "    inputs: A tensor of size [batch, height, width, channels].\n",
        "    depth: The depth of the ResNet unit output.\n",
        "    depth_bottleneck: The depth of the bottleneck layers.\n",
        "    stride: The ResNet unit's stride. Determines the amount of downsampling of\n",
        "      the units output compared to its input.\n",
        "    rate: An integer, rate for atrous convolution.\n",
        "    outputs_collections: Collection to add the ResNet unit output.\n",
        "    scope: Optional variable_scope.\n",
        "  Returns:\n",
        "    The ResNet unit's output.\n",
        "  \"\"\"\n",
        "  with tf.variable_scope(scope, 'bottleneck_v2', [inputs]) as sc:\n",
        "    depth_in = slim.utils.last_dimension(inputs.get_shape(), min_rank=4)\n",
        "    preact = slim.batch_norm(inputs, activation_fn=tf.nn.relu, scope='preact')\n",
        "    if depth == depth_in:\n",
        "      shortcut = resnet_utils.subsample(inputs, stride, 'shortcut')\n",
        "    else:\n",
        "      shortcut = slim.conv2d(preact, depth, [1, 1], stride=stride,\n",
        "                             normalizer_fn=None, activation_fn=None,\n",
        "                             scope='shortcut')\n",
        "\n",
        "    residual = slim.conv2d(preact, depth_bottleneck, [1, 1], stride=1,\n",
        "                           scope='conv1')\n",
        "    residual = resnet_utils.conv2d_same(residual, depth_bottleneck, 3, stride,\n",
        "                                        rate=rate, scope='conv2')\n",
        "    residual = slim.conv2d(residual, depth, [1, 1], stride=1,\n",
        "                           normalizer_fn=None, activation_fn=None,\n",
        "                           scope='conv3')\n",
        "\n",
        "    output = shortcut + residual\n",
        "\n",
        "    return slim.utils.collect_named_outputs(outputs_collections,\n",
        "                                            sc.original_name_scope,\n",
        "                                            output)\n",
        "\n",
        "\n",
        "def resnet_v2(inputs,\n",
        "              blocks,\n",
        "              num_classes=None,\n",
        "              is_training=True,\n",
        "              global_pool=True,\n",
        "              output_stride=None,\n",
        "              include_root_block=True,\n",
        "              spatial_squeeze=True,\n",
        "              reuse=None,\n",
        "              scope=None):\n",
        "  \"\"\"Generator for v2 (preactivation) ResNet models.\n",
        "  This function generates a family of ResNet v2 models. See the resnet_v2_*()\n",
        "  methods for specific model instantiations, obtained by selecting different\n",
        "  block instantiations that produce ResNets of various depths.\n",
        "  Training for image classification on Imagenet is usually done with [224, 224]\n",
        "  inputs, resulting in [7, 7] feature maps at the output of the last ResNet\n",
        "  block for the ResNets defined in [1] that have nominal stride equal to 32.\n",
        "  However, for dense prediction tasks we advise that one uses inputs with\n",
        "  spatial dimensions that are multiples of 32 plus 1, e.g., [321, 321]. In\n",
        "  this case the feature maps at the ResNet output will have spatial shape\n",
        "  [(height - 1) / output_stride + 1, (width - 1) / output_stride + 1]\n",
        "  and corners exactly aligned with the input image corners, which greatly\n",
        "  facilitates alignment of the features to the image. Using as input [225, 225]\n",
        "  images results in [8, 8] feature maps at the output of the last ResNet block.\n",
        "  For dense prediction tasks, the ResNet needs to run in fully-convolutional\n",
        "  (FCN) mode and global_pool needs to be set to False. The ResNets in [1, 2] all\n",
        "  have nominal stride equal to 32 and a good choice in FCN mode is to use\n",
        "  output_stride=16 in order to increase the density of the computed features at\n",
        "  small computational and memory overhead, cf. http://arxiv.org/abs/1606.00915.\n",
        "  Args:\n",
        "    inputs: A tensor of size [batch, height_in, width_in, channels].\n",
        "    blocks: A list of length equal to the number of ResNet blocks. Each element\n",
        "      is a resnet_utils.Block object describing the units in the block.\n",
        "    num_classes: Number of predicted classes for classification tasks. If None\n",
        "      we return the features before the logit layer.\n",
        "    is_training: whether is training or not.\n",
        "    global_pool: If True, we perform global average pooling before computing the\n",
        "      logits. Set to True for image classification, False for dense prediction.\n",
        "    output_stride: If None, then the output will be computed at the nominal\n",
        "      network stride. If output_stride is not None, it specifies the requested\n",
        "      ratio of input to output spatial resolution.\n",
        "    include_root_block: If True, include the initial convolution followed by\n",
        "      max-pooling, if False excludes it. If excluded, `inputs` should be the\n",
        "      results of an activation-less convolution.\n",
        "    spatial_squeeze: if True, logits is of shape [B, C], if false logits is\n",
        "        of shape [B, 1, 1, C], where B is batch_size and C is number of classes.\n",
        "    reuse: whether or not the network and its variables should be reused. To be\n",
        "      able to reuse 'scope' must be given.\n",
        "    scope: Optional variable_scope.\n",
        "  Returns:\n",
        "    net: A rank-4 tensor of size [batch, height_out, width_out, channels_out].\n",
        "      If global_pool is False, then height_out and width_out are reduced by a\n",
        "      factor of output_stride compared to the respective height_in and width_in,\n",
        "      else both height_out and width_out equal one. If num_classes is None, then\n",
        "      net is the output of the last ResNet block, potentially after global\n",
        "      average pooling. If num_classes is not None, net contains the pre-softmax\n",
        "      activations.\n",
        "    end_points: A dictionary from components of the network to the corresponding\n",
        "      activation.\n",
        "  Raises:\n",
        "    ValueError: If the target output_stride is not valid.\n",
        "  \"\"\"\n",
        "  with tf.variable_scope(scope, 'resnet_v2', [inputs], reuse=reuse) as sc:\n",
        "    end_points_collection = sc.name + '_end_points'\n",
        "    with slim.arg_scope([slim.conv2d, bottleneck,\n",
        "                         resnet_utils.stack_blocks_dense],\n",
        "                        outputs_collections=end_points_collection):\n",
        "      with slim.arg_scope([slim.batch_norm], is_training=is_training):\n",
        "        net = inputs\n",
        "        if include_root_block:\n",
        "          if output_stride is not None:\n",
        "            if output_stride % 4 != 0:\n",
        "              raise ValueError('The output_stride needs to be a multiple of 4.')\n",
        "            output_stride /= 4\n",
        "          # We do not include batch normalization or activation functions in\n",
        "          # conv1 because the first ResNet unit will perform these. Cf.\n",
        "          # Appendix of [2].\n",
        "          with slim.arg_scope([slim.conv2d],\n",
        "                              activation_fn=None, normalizer_fn=None):\n",
        "            net = resnet_utils.conv2d_same(net, 64, 7, stride=2, scope='conv1')\n",
        "          net = slim.max_pool2d(net, [3, 3], stride=2, scope='pool1')\n",
        "        net = resnet_utils.stack_blocks_dense(net, blocks, output_stride)\n",
        "        # This is needed because the pre-activation variant does not have batch\n",
        "        # normalization or activation functions in the residual unit output. See\n",
        "        # Appendix of [2].\n",
        "        net = slim.batch_norm(net, activation_fn=tf.nn.relu, scope='postnorm')\n",
        "        if global_pool:\n",
        "          # Global average pooling.\n",
        "          net = tf.reduce_mean(net, [1, 2], name='pool5', keep_dims=True)\n",
        "        if num_classes is not None:\n",
        "          net = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,\n",
        "                            normalizer_fn=None, scope='logits')\n",
        "        if spatial_squeeze:\n",
        "          logits = tf.squeeze(net, [1, 2], name='SpatialSqueeze')\n",
        "        # Convert end_points_collection into a dictionary of end_points.\n",
        "        end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n",
        "        if num_classes is not None:\n",
        "          end_points['predictions'] = slim.softmax(logits, scope='predictions')\n",
        "        return logits, end_points\n",
        "resnet_v2.default_image_size = 224\n",
        "\n",
        "\n",
        "def resnet_v2_50(inputs,\n",
        "                 num_classes=None,\n",
        "                 is_training=True,\n",
        "                 global_pool=True,\n",
        "                 output_stride=None,\n",
        "                 reuse=None,\n",
        "                 scope='resnet_v2_50'):\n",
        "  \"\"\"ResNet-50 model of [1]. See resnet_v2() for arg and return description.\"\"\"\n",
        "  blocks = [\n",
        "      resnet_utils.Block(\n",
        "          'block1', bottleneck, [(256, 64, 1)] * 2 + [(256, 64, 2)]),\n",
        "      resnet_utils.Block(\n",
        "          'block2', bottleneck, [(512, 128, 1)] * 3 + [(512, 128, 2)]),\n",
        "      resnet_utils.Block(\n",
        "          'block3', bottleneck, [(1024, 256, 1)] * 5 + [(1024, 256, 2)]),\n",
        "      resnet_utils.Block(\n",
        "          'block4', bottleneck, [(2048, 512, 1)] * 3)]\n",
        "  return resnet_v2(inputs, blocks, num_classes, is_training=is_training,\n",
        "                   global_pool=global_pool, output_stride=output_stride,\n",
        "                   include_root_block=True, reuse=reuse, scope=scope)\n",
        "resnet_v2_50.default_image_size = resnet_v2.default_image_size\n",
        "\n",
        "\n",
        "def resnet_v2_101(inputs,\n",
        "                  num_classes=None,\n",
        "                  is_training=True,\n",
        "                  global_pool=True,\n",
        "                  output_stride=None,\n",
        "                  reuse=None,\n",
        "                  scope='resnet_v2_101'):\n",
        "  \"\"\"ResNet-101 model of [1]. See resnet_v2() for arg and return description.\"\"\"\n",
        "  blocks = [\n",
        "      resnet_utils.Block(\n",
        "          'block1', bottleneck, [(256, 64, 1)] * 2 + [(256, 64, 2)]),\n",
        "      resnet_utils.Block(\n",
        "          'block2', bottleneck, [(512, 128, 1)] * 3 + [(512, 128, 2)]),\n",
        "      resnet_utils.Block(\n",
        "          'block3', bottleneck, [(1024, 256, 1)] * 22 + [(1024, 256, 2)]),\n",
        "      resnet_utils.Block(\n",
        "          'block4', bottleneck, [(2048, 512, 1)] * 3)]\n",
        "  return resnet_v2(inputs, blocks, num_classes, is_training=is_training,\n",
        "                   global_pool=global_pool, output_stride=output_stride,\n",
        "                   include_root_block=True, reuse=reuse, scope=scope)\n",
        "resnet_v2_101.default_image_size = resnet_v2.default_image_size\n",
        "\n",
        "\n",
        "def resnet_v2_152(inputs,\n",
        "                  num_classes=None,\n",
        "                  is_training=True,\n",
        "                  global_pool=True,\n",
        "                  output_stride=None,\n",
        "                  reuse=None,\n",
        "                  scope='resnet_v2_152'):\n",
        "  \"\"\"ResNet-152 model of [1]. See resnet_v2() for arg and return description.\"\"\"\n",
        "  blocks = [\n",
        "      resnet_utils.Block(\n",
        "          'block1', bottleneck, [(256, 64, 1)] * 2 + [(256, 64, 2)]),\n",
        "      resnet_utils.Block(\n",
        "          'block2', bottleneck, [(512, 128, 1)] * 7 + [(512, 128, 2)]),\n",
        "      resnet_utils.Block(\n",
        "          'block3', bottleneck, [(1024, 256, 1)] * 35 + [(1024, 256, 2)]),\n",
        "      resnet_utils.Block(\n",
        "          'block4', bottleneck, [(2048, 512, 1)] * 3)]\n",
        "  return resnet_v2(inputs, blocks, num_classes, is_training=is_training,\n",
        "                   global_pool=global_pool, output_stride=output_stride,\n",
        "                   include_root_block=True, reuse=reuse, scope=scope)\n",
        "resnet_v2_152.default_image_size = resnet_v2.default_image_size\n",
        "\n",
        "\n",
        "def resnet_v2_200(inputs,\n",
        "                  num_classes=None,\n",
        "                  is_training=True,\n",
        "                  global_pool=True,\n",
        "                  output_stride=None,\n",
        "                  reuse=None,\n",
        "                  scope='resnet_v2_200'):\n",
        "  \"\"\"ResNet-200 model of [2]. See resnet_v2() for arg and return description.\"\"\"\n",
        "  blocks = [\n",
        "      resnet_utils.Block(\n",
        "          'block1', bottleneck, [(256, 64, 1)] * 2 + [(256, 64, 2)]),\n",
        "      resnet_utils.Block(\n",
        "          'block2', bottleneck, [(512, 128, 1)] * 23 + [(512, 128, 2)]),\n",
        "      resnet_utils.Block(\n",
        "          'block3', bottleneck, [(1024, 256, 1)] * 35 + [(1024, 256, 2)]),\n",
        "      resnet_utils.Block(\n",
        "          'block4', bottleneck, [(2048, 512, 1)] * 3)]\n",
        "  return resnet_v2(inputs, blocks, num_classes, is_training=is_training,\n",
        "                   global_pool=global_pool, output_stride=output_stride,\n",
        "                   include_root_block=True, reuse=reuse, scope=scope)\n",
        "resnet_v2_200.default_image_size = resnet_v2.default_image_size"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}