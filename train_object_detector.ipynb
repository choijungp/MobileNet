{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\"\"\"Generic training script that trains a model using a given dataset.\"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.python.ops import control_flow_ops\n",
        "from datasets import dataset_factory\n",
        "from deployment import model_deploy\n",
        "from nets import nets_factory\n",
        "from preprocessing import preprocessing_factory\n",
        "\n",
        "from utils.det_utils import encode_annos, losses, interpre_prediction\n",
        "\n",
        "from configs.kitti_config import config\n",
        "\n",
        "import tensorflow.contrib.slim as slim\n",
        "\n",
        "# slim = tf.contrib.slim\n",
        "\n",
        "tf.app.flags.DEFINE_string(\n",
        "  'master', '', 'The address of the TensorFlow master to use.')\n",
        "\n",
        "tf.app.flags.DEFINE_string(\n",
        "  'train_dir', '/tmp/tfmodel/',\n",
        "  'Directory where checkpoints and event logs are written to.')\n",
        "\n",
        "tf.app.flags.DEFINE_integer('num_clones', 1,\n",
        "                            'Number of model clones to deploy.')\n",
        "\n",
        "tf.app.flags.DEFINE_boolean('clone_on_cpu', False,\n",
        "                            'Use CPUs to deploy clones.')\n",
        "\n",
        "tf.app.flags.DEFINE_integer('worker_replicas', 1, 'Number of worker replicas.')\n",
        "\n",
        "tf.app.flags.DEFINE_integer(\n",
        "  'num_ps_tasks', 0,\n",
        "  'The number of parameter servers. If the value is 0, then the parameters '\n",
        "  'are handled locally by the worker.')\n",
        "\n",
        "tf.app.flags.DEFINE_integer(\n",
        "  'num_readers', 4,\n",
        "  'The number of parallel readers that read data from the dataset.')\n",
        "\n",
        "tf.app.flags.DEFINE_integer(\n",
        "  'num_preprocessing_threads', 4,\n",
        "  'The number of threads used to create the batches.')\n",
        "\n",
        "tf.app.flags.DEFINE_integer(\n",
        "  'log_every_n_steps', 10,\n",
        "  'The frequency with which logs are print.')\n",
        "\n",
        "tf.app.flags.DEFINE_integer(\n",
        "  'save_summaries_secs', 600,\n",
        "  'The frequency with which summaries are saved, in seconds.')\n",
        "\n",
        "tf.app.flags.DEFINE_integer(\n",
        "  'save_interval_secs', 600,\n",
        "  'The frequency with which the model is saved, in seconds.')\n",
        "\n",
        "tf.app.flags.DEFINE_integer(\n",
        "  'task', 0, 'Task id of the replica running the training.')\n",
        "\n",
        "######################\n",
        "# Optimization Flags #\n",
        "######################\n",
        "\n",
        "tf.app.flags.DEFINE_float(\n",
        "  'weight_decay', 0.00004, 'The weight decay on the model weights.')\n",
        "\n",
        "tf.app.flags.DEFINE_string(\n",
        "  'optimizer', 'rmsprop',\n",
        "  'The name of the optimizer, one of \"adadelta\", \"adagrad\", \"adam\",'\n",
        "  '\"ftrl\", \"momentum\", \"sgd\" or \"rmsprop\".')\n",
        "\n",
        "tf.app.flags.DEFINE_float(\n",
        "  'adadelta_rho', 0.95,\n",
        "  'The decay rate for adadelta.')\n",
        "\n",
        "tf.app.flags.DEFINE_float(\n",
        "  'adagrad_initial_accumulator_value', 0.1,\n",
        "  'Starting value for the AdaGrad accumulators.')\n",
        "\n",
        "tf.app.flags.DEFINE_float(\n",
        "  'adam_beta1', 0.9,\n",
        "  'The exponential decay rate for the 1st moment estimates.')\n",
        "\n",
        "tf.app.flags.DEFINE_float(\n",
        "  'adam_beta2', 0.999,\n",
        "  'The exponential decay rate for the 2nd moment estimates.')\n",
        "\n",
        "tf.app.flags.DEFINE_float('opt_epsilon', 1.0, 'Epsilon term for the optimizer.')\n",
        "\n",
        "tf.app.flags.DEFINE_float('ftrl_learning_rate_power', -0.5,\n",
        "                          'The learning rate power.')\n",
        "\n",
        "tf.app.flags.DEFINE_float(\n",
        "  'ftrl_initial_accumulator_value', 0.1,\n",
        "  'Starting value for the FTRL accumulators.')\n",
        "\n",
        "tf.app.flags.DEFINE_float(\n",
        "  'ftrl_l1', 0.0, 'The FTRL l1 regularization strength.')\n",
        "\n",
        "tf.app.flags.DEFINE_float(\n",
        "  'ftrl_l2', 0.0, 'The FTRL l2 regularization strength.')\n",
        "\n",
        "tf.app.flags.DEFINE_float(\n",
        "  'momentum', 0.9,\n",
        "  'The momentum for the MomentumOptimizer and RMSPropOptimizer.')\n",
        "\n",
        "tf.app.flags.DEFINE_float('rmsprop_momentum', 0.9, 'Momentum.')\n",
        "\n",
        "tf.app.flags.DEFINE_float('rmsprop_decay', 0.9, 'Decay term for RMSProp.')\n",
        "\n",
        "#######################\n",
        "# Learning Rate Flags #\n",
        "#######################\n",
        "\n",
        "tf.app.flags.DEFINE_string(\n",
        "  'learning_rate_decay_type',\n",
        "  'exponential',\n",
        "  'Specifies how the learning rate is decayed. One of \"fixed\", \"exponential\",'\n",
        "  ' or \"polynomial\"')\n",
        "\n",
        "tf.app.flags.DEFINE_float('learning_rate', 0.01, 'Initial learning rate.')\n",
        "\n",
        "tf.app.flags.DEFINE_float(\n",
        "  'end_learning_rate', 0.0001,\n",
        "  'The minimal end learning rate used by a polynomial decay learning rate.')\n",
        "\n",
        "tf.app.flags.DEFINE_float(\n",
        "  'label_smoothing', 0.0, 'The amount of label smoothing.')\n",
        "\n",
        "tf.app.flags.DEFINE_float(\n",
        "  'learning_rate_decay_factor', 0.94, 'Learning rate decay factor.')\n",
        "\n",
        "tf.app.flags.DEFINE_float(\n",
        "  'num_epochs_per_decay', 2.0,\n",
        "  'Number of epochs after which learning rate decays.')\n",
        "\n",
        "tf.app.flags.DEFINE_bool(\n",
        "  'sync_replicas', False,\n",
        "  'Whether or not to synchronize the replicas during training.')\n",
        "\n",
        "tf.app.flags.DEFINE_integer(\n",
        "  'replicas_to_aggregate', 1,\n",
        "  'The Number of gradients to collect before updating params.')\n",
        "\n",
        "tf.app.flags.DEFINE_float(\n",
        "  'moving_average_decay', None,\n",
        "  'The decay to use for the moving average.'\n",
        "  'If left as None, then moving averages are not used.')\n",
        "\n",
        "#######################\n",
        "# Dataset Flags #\n",
        "#######################\n",
        "\n",
        "tf.app.flags.DEFINE_string(\n",
        "  'dataset_name', 'imagenet', 'The name of the dataset to load.')\n",
        "\n",
        "tf.app.flags.DEFINE_string(\n",
        "  'dataset_split_name', 'train', 'The name of the train/test split.')\n",
        "\n",
        "tf.app.flags.DEFINE_string(\n",
        "  'dataset_dir', None, 'The directory where the dataset files are stored.')\n",
        "\n",
        "tf.app.flags.DEFINE_integer(\n",
        "  'labels_offset', 0,\n",
        "  'An offset for the labels in the dataset. This flag is primarily used to '\n",
        "  'evaluate the VGG and ResNet architectures which do not use a background '\n",
        "  'class for the ImageNet dataset.')\n",
        "\n",
        "tf.app.flags.DEFINE_string(\n",
        "  'model_name', 'inception_v3', 'The name of the architecture to train.')\n",
        "\n",
        "tf.app.flags.DEFINE_string(\n",
        "  'preprocessing_name', None, 'The name of the preprocessing to use. If left '\n",
        "                              'as `None`, then the model_name flag is used.')\n",
        "\n",
        "tf.app.flags.DEFINE_integer(\n",
        "  'batch_size', 32, 'The number of samples in each batch.')\n",
        "\n",
        "tf.app.flags.DEFINE_integer(\n",
        "  'train_image_size', None, 'Train image size')\n",
        "\n",
        "tf.app.flags.DEFINE_integer('max_number_of_steps', None,\n",
        "                            'The maximum number of training steps.')\n",
        "\n",
        "tf.app.flags.DEFINE_float('width_multiplier', 1.0,\n",
        "                          'Width Multiplier, for MobileNet only.')\n",
        "\n",
        "#####################\n",
        "# Fine-Tuning Flags #\n",
        "#####################\n",
        "\n",
        "tf.app.flags.DEFINE_string(\n",
        "  'checkpoint_path', None,\n",
        "  'The path to a checkpoint from which to fine-tune.')\n",
        "\n",
        "tf.app.flags.DEFINE_string(\n",
        "  'checkpoint_exclude_scopes', None,\n",
        "  'Comma-separated list of scopes of variables to exclude when restoring '\n",
        "  'from a checkpoint.')\n",
        "\n",
        "tf.app.flags.DEFINE_string(\n",
        "  'trainable_scopes', None,\n",
        "  'Comma-separated list of scopes to filter the set of variables to train.'\n",
        "  'By default, None would train all the variables.')\n",
        "\n",
        "tf.app.flags.DEFINE_boolean(\n",
        "  'ignore_missing_vars', False,\n",
        "  'When restoring a checkpoint would ignore missing variables.')\n",
        "\n",
        "FLAGS = tf.app.flags.FLAGS\n",
        "\n",
        "\n",
        "def _configure_learning_rate(num_samples_per_epoch, global_step):\n",
        "  \"\"\"Configures the learning rate.\n",
        "  Args:\n",
        "    num_samples_per_epoch: The number of samples in each epoch of training.\n",
        "    global_step: The global_step tensor.\n",
        "  Returns:\n",
        "    A `Tensor` representing the learning rate.\n",
        "  Raises:\n",
        "    ValueError: if\n",
        "  \"\"\"\n",
        "  decay_steps = int(num_samples_per_epoch / FLAGS.batch_size *\n",
        "                    FLAGS.num_epochs_per_decay)\n",
        "  if FLAGS.sync_replicas:\n",
        "    decay_steps /= FLAGS.replicas_to_aggregate\n",
        "\n",
        "  if FLAGS.learning_rate_decay_type == 'exponential':\n",
        "    return tf.train.exponential_decay(FLAGS.learning_rate,\n",
        "                                      global_step,\n",
        "                                      decay_steps,\n",
        "                                      FLAGS.learning_rate_decay_factor,\n",
        "                                      staircase=True,\n",
        "                                      name='exponential_decay_learning_rate')\n",
        "  elif FLAGS.learning_rate_decay_type == 'fixed':\n",
        "    return tf.constant(FLAGS.learning_rate, name='fixed_learning_rate')\n",
        "  elif FLAGS.learning_rate_decay_type == 'polynomial':\n",
        "    return tf.train.polynomial_decay(FLAGS.learning_rate,\n",
        "                                     global_step,\n",
        "                                     decay_steps,\n",
        "                                     FLAGS.end_learning_rate,\n",
        "                                     power=1.0,\n",
        "                                     cycle=False,\n",
        "                                     name='polynomial_decay_learning_rate')\n",
        "  else:\n",
        "    raise ValueError('learning_rate_decay_type [%s] was not recognized',\n",
        "                     FLAGS.learning_rate_decay_type)\n",
        "\n",
        "\n",
        "def _configure_optimizer(learning_rate):\n",
        "  \"\"\"Configures the optimizer used for training.\n",
        "  Args:\n",
        "    learning_rate: A scalar or `Tensor` learning rate.\n",
        "  Returns:\n",
        "    An instance of an optimizer.\n",
        "  Raises:\n",
        "    ValueError: if FLAGS.optimizer is not recognized.\n",
        "  \"\"\"\n",
        "  if FLAGS.optimizer == 'adadelta':\n",
        "    optimizer = tf.train.AdadeltaOptimizer(\n",
        "      learning_rate,\n",
        "      rho=FLAGS.adadelta_rho,\n",
        "      epsilon=FLAGS.opt_epsilon)\n",
        "  elif FLAGS.optimizer == 'adagrad':\n",
        "    optimizer = tf.train.AdagradOptimizer(\n",
        "      learning_rate,\n",
        "      initial_accumulator_value=FLAGS.adagrad_initial_accumulator_value)\n",
        "  elif FLAGS.optimizer == 'adam':\n",
        "    optimizer = tf.train.AdamOptimizer(\n",
        "      learning_rate,\n",
        "      beta1=FLAGS.adam_beta1,\n",
        "      beta2=FLAGS.adam_beta2,\n",
        "      epsilon=FLAGS.opt_epsilon)\n",
        "  elif FLAGS.optimizer == 'ftrl':\n",
        "    optimizer = tf.train.FtrlOptimizer(\n",
        "      learning_rate,\n",
        "      learning_rate_power=FLAGS.ftrl_learning_rate_power,\n",
        "      initial_accumulator_value=FLAGS.ftrl_initial_accumulator_value,\n",
        "      l1_regularization_strength=FLAGS.ftrl_l1,\n",
        "      l2_regularization_strength=FLAGS.ftrl_l2)\n",
        "  elif FLAGS.optimizer == 'momentum':\n",
        "    optimizer = tf.train.MomentumOptimizer(\n",
        "      learning_rate,\n",
        "      momentum=FLAGS.momentum,\n",
        "      name='Momentum')\n",
        "  elif FLAGS.optimizer == 'rmsprop':\n",
        "    optimizer = tf.train.RMSPropOptimizer(\n",
        "      learning_rate,\n",
        "      decay=FLAGS.rmsprop_decay,\n",
        "      momentum=FLAGS.rmsprop_momentum,\n",
        "      epsilon=FLAGS.opt_epsilon)\n",
        "  elif FLAGS.optimizer == 'sgd':\n",
        "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
        "  else:\n",
        "    raise ValueError('Optimizer [%s] was not recognized', FLAGS.optimizer)\n",
        "  return optimizer\n",
        "\n",
        "\n",
        "def _add_variables_summaries(learning_rate):\n",
        "  summaries = []\n",
        "  for variable in slim.get_model_variables():\n",
        "    summaries.append(tf.summary.histogram(variable.op.name, variable))\n",
        "  summaries.append(tf.summary.scalar('training/Learning Rate', learning_rate))\n",
        "  return summaries\n",
        "\n",
        "\n",
        "def _get_init_fn():\n",
        "  \"\"\"Returns a function run by the chief worker to warm-start the training.\n",
        "  Note that the init_fn is only run when initializing the model during the very\n",
        "  first global step.\n",
        "  Returns:\n",
        "    An init function run by the supervisor.\n",
        "  \"\"\"\n",
        "  if FLAGS.checkpoint_path is None:\n",
        "    return None\n",
        "\n",
        "  # Warn the user if a checkpoint exists in the train_dir. Then we'll be\n",
        "  # ignoring the checkpoint anyway.\n",
        "  if tf.train.latest_checkpoint(FLAGS.train_dir):\n",
        "    tf.logging.info(\n",
        "      'Ignoring --checkpoint_path because a checkpoint already exists in %s'\n",
        "      % FLAGS.train_dir)\n",
        "    return None\n",
        "\n",
        "  exclusions = []\n",
        "  if FLAGS.checkpoint_exclude_scopes:\n",
        "    exclusions = [scope.strip()\n",
        "                  for scope in FLAGS.checkpoint_exclude_scopes.split(',')]\n",
        "\n",
        "  # TODO(sguada) variables.filter_variables()\n",
        "  variables_to_restore = []\n",
        "  for var in slim.get_model_variables():\n",
        "    excluded = False\n",
        "    for exclusion in exclusions:\n",
        "      if var.op.name.startswith(exclusion):\n",
        "        excluded = True\n",
        "        break\n",
        "    if not excluded:\n",
        "      variables_to_restore.append(var)\n",
        "\n",
        "  if tf.gfile.IsDirectory(FLAGS.checkpoint_path):\n",
        "    checkpoint_path = tf.train.latest_checkpoint(FLAGS.checkpoint_path)\n",
        "  else:\n",
        "    checkpoint_path = FLAGS.checkpoint_path\n",
        "\n",
        "  tf.logging.info('Fine-tuning from %s' % checkpoint_path)\n",
        "\n",
        "  return slim.assign_from_checkpoint_fn(\n",
        "    checkpoint_path,\n",
        "    variables_to_restore,\n",
        "    ignore_missing_vars=FLAGS.ignore_missing_vars)\n",
        "\n",
        "\n",
        "def _get_variables_to_train():\n",
        "  \"\"\"Returns a list of variables to train.\n",
        "  Returns:\n",
        "    A list of variables to train by the optimizer.\n",
        "  \"\"\"\n",
        "  if FLAGS.trainable_scopes is None:\n",
        "    return tf.trainable_variables()\n",
        "  else:\n",
        "    scopes = [scope.strip() for scope in FLAGS.trainable_scopes.split(',')]\n",
        "\n",
        "  variables_to_train = []\n",
        "  for scope in scopes:\n",
        "    variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope)\n",
        "    variables_to_train.extend(variables)\n",
        "  return variables_to_train\n",
        "\n",
        "\n",
        "def main(_):\n",
        "  if not FLAGS.dataset_dir:\n",
        "    raise ValueError('You must supply the dataset directory with --dataset_dir')\n",
        "\n",
        "  tf.logging.set_verbosity(tf.logging.INFO)\n",
        "  with tf.Graph().as_default():\n",
        "    #######################\n",
        "    # Config model_deploy #\n",
        "    #######################\n",
        "    deploy_config = model_deploy.DeploymentConfig(\n",
        "      num_clones=FLAGS.num_clones,\n",
        "      clone_on_cpu=FLAGS.clone_on_cpu,\n",
        "      replica_id=FLAGS.task,\n",
        "      num_replicas=FLAGS.worker_replicas,\n",
        "      num_ps_tasks=FLAGS.num_ps_tasks)\n",
        "\n",
        "    # Create global_step\n",
        "    with tf.device(deploy_config.variables_device()):\n",
        "      global_step = slim.create_global_step()\n",
        "\n",
        "    ######################\n",
        "    # Select the dataset #\n",
        "    ######################\n",
        "    dataset = dataset_factory.get_dataset(\n",
        "      FLAGS.dataset_name, FLAGS.dataset_split_name, FLAGS.dataset_dir)\n",
        "\n",
        "    ######################\n",
        "    # Select the network #\n",
        "    ######################\n",
        "    network_fn = nets_factory.get_network_fn(\n",
        "      FLAGS.model_name,\n",
        "      num_classes=(dataset.num_classes - FLAGS.labels_offset),\n",
        "      weight_decay=FLAGS.weight_decay,\n",
        "      is_training=True,\n",
        "      width_multiplier=FLAGS.width_multiplier)\n",
        "\n",
        "    #####################################\n",
        "    # Select the preprocessing function #\n",
        "    #####################################\n",
        "    preprocessing_name = FLAGS.preprocessing_name or FLAGS.model_name\n",
        "    image_preprocessing_fn = preprocessing_factory.get_preprocessing(\n",
        "      preprocessing_name,\n",
        "      is_training=True)\n",
        "\n",
        "    ##############################################################\n",
        "    # Create a dataset provider that loads data from the dataset #\n",
        "    ##############################################################\n",
        "    with tf.device(deploy_config.inputs_device()):\n",
        "      provider = slim.dataset_data_provider.DatasetDataProvider(\n",
        "        dataset,\n",
        "        num_readers=FLAGS.num_readers,\n",
        "        common_queue_capacity=20 * FLAGS.batch_size,\n",
        "        common_queue_min=10 * FLAGS.batch_size)\n",
        "\n",
        "      # gt_bboxes format [ymin, xmin, ymax, xmax]\n",
        "      [image, img_shape, gt_labels, gt_bboxes] = provider.get(['image', 'shape',\n",
        "                                                               'object/label',\n",
        "                                                               'object/bbox'])\n",
        "\n",
        "      # Preprocesing\n",
        "      # gt_bboxes = scale_bboxes(gt_bboxes, img_shape)  # bboxes format [0,1) for tf draw\n",
        "\n",
        "      image, gt_labels, gt_bboxes = image_preprocessing_fn(image,\n",
        "                                                           config.IMG_HEIGHT,\n",
        "                                                           config.IMG_WIDTH,\n",
        "                                                           labels=gt_labels,\n",
        "                                                           bboxes=gt_bboxes,\n",
        "                                                           )\n",
        "\n",
        "      #############################################\n",
        "      # Encode annotations for losses computation #\n",
        "      #############################################\n",
        "\n",
        "      # anchors format [cx, cy, w, h]\n",
        "      anchors = tf.convert_to_tensor(config.ANCHOR_SHAPE, dtype=tf.float32)\n",
        "\n",
        "      # encode annos, box_input format [cx, cy, w, h]\n",
        "      input_mask, labels_input, box_delta_input, box_input = encode_annos(gt_labels,\n",
        "                                                                          gt_bboxes,\n",
        "                                                                          anchors,\n",
        "                                                                          config.NUM_CLASSES)\n",
        "\n",
        "      images, b_input_mask, b_labels_input, b_box_delta_input, b_box_input = tf.train.batch(\n",
        "        [image, input_mask, labels_input, box_delta_input, box_input],\n",
        "        batch_size=FLAGS.batch_size,\n",
        "        num_threads=FLAGS.num_preprocessing_threads,\n",
        "        capacity=5 * FLAGS.batch_size)\n",
        "\n",
        "      batch_queue = slim.prefetch_queue.prefetch_queue(\n",
        "        [images, b_input_mask, b_labels_input, b_box_delta_input, b_box_input], capacity=2 * deploy_config.num_clones)\n",
        "\n",
        "    ####################\n",
        "    # Define the model #\n",
        "    ####################\n",
        "    def clone_fn(batch_queue):\n",
        "      \"\"\"Allows data parallelism by creating multiple clones of network_fn.\"\"\"\n",
        "      images, b_input_mask, b_labels_input, b_box_delta_input, b_box_input = batch_queue.dequeue()\n",
        "      anchors = tf.convert_to_tensor(config.ANCHOR_SHAPE, dtype=tf.float32)\n",
        "      end_points = network_fn(images)\n",
        "      end_points[\"viz_images\"] = images\n",
        "      conv_ds_14 = end_points['MobileNet/conv_ds_14/depthwise_conv']\n",
        "      dropout = slim.dropout(conv_ds_14, keep_prob=0.5, is_training=True)\n",
        "      num_output = config.NUM_ANCHORS * (config.NUM_CLASSES + 1 + 4)\n",
        "      predict = slim.conv2d(dropout, num_output, kernel_size=(3, 3), stride=1, padding='SAME',\n",
        "                            activation_fn=None,\n",
        "                            weights_initializer=tf.truncated_normal_initializer(stddev=0.0001),\n",
        "                            scope=\"MobileNet/conv_predict\")\n",
        "\n",
        "      with tf.name_scope(\"Interpre_prediction\") as scope:\n",
        "        pred_box_delta, pred_class_probs, pred_conf, ious, det_probs, det_boxes, det_class = \\\n",
        "          interpre_prediction(predict, b_input_mask, anchors, b_box_input)\n",
        "        end_points[\"viz_det_probs\"] = det_probs\n",
        "        end_points[\"viz_det_boxes\"] = det_boxes\n",
        "        end_points[\"viz_det_class\"] = det_class\n",
        "\n",
        "      with tf.name_scope(\"Losses\") as scope:\n",
        "        losses(b_input_mask, b_labels_input, ious, b_box_delta_input, pred_class_probs, pred_conf, pred_box_delta)\n",
        "\n",
        "      return end_points\n",
        "\n",
        "    # Gather initial summaries.\n",
        "    summaries = set(tf.get_collection(tf.GraphKeys.SUMMARIES))\n",
        "\n",
        "    clones = model_deploy.create_clones(deploy_config, clone_fn, [batch_queue])\n",
        "    first_clone_scope = deploy_config.clone_scope(0)\n",
        "    # Gather update_ops from the first clone. These contain, for example,\n",
        "    # the updates for the batch_norm variables created by network_fn.\n",
        "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, first_clone_scope)\n",
        "\n",
        "    # Add summaries for end_points.\n",
        "    end_points = clones[0].outputs\n",
        "    for end_point in end_points:\n",
        "      if end_point not in [\"viz_images\", \"viz_det_probs\", \"viz_det_boxes\", \"viz_det_class\"]:\n",
        "        x = end_points[end_point]\n",
        "        summaries.add(tf.summary.histogram('activations/' + end_point, x))\n",
        "        summaries.add(tf.summary.scalar('sparsity/' + end_point,\n",
        "                                        tf.nn.zero_fraction(x)))\n",
        "\n",
        "    # Add summaries for det result TODO(shizehao): vizulize prediction\n",
        "\n",
        "\n",
        "    # Add summaries for losses.\n",
        "    for loss in tf.get_collection(tf.GraphKeys.LOSSES, first_clone_scope):\n",
        "      summaries.add(tf.summary.scalar('losses/%s' % loss.op.name, loss))\n",
        "\n",
        "    # Add summaries for variables.\n",
        "    for variable in slim.get_model_variables():\n",
        "      summaries.add(tf.summary.histogram(variable.op.name, variable))\n",
        "\n",
        "    #################################\n",
        "    # Configure the moving averages #\n",
        "    #################################\n",
        "    if FLAGS.moving_average_decay:\n",
        "      moving_average_variables = slim.get_model_variables()\n",
        "      variable_averages = tf.train.ExponentialMovingAverage(\n",
        "        FLAGS.moving_average_decay, global_step)\n",
        "    else:\n",
        "      moving_average_variables, variable_averages = None, None\n",
        "\n",
        "    #########################################\n",
        "    # Configure the optimization procedure. #\n",
        "    #########################################\n",
        "    with tf.device(deploy_config.optimizer_device()):\n",
        "      learning_rate = _configure_learning_rate(dataset.num_samples, global_step)\n",
        "      optimizer = _configure_optimizer(learning_rate)\n",
        "      summaries.add(tf.summary.scalar('learning_rate', learning_rate))\n",
        "\n",
        "    if FLAGS.sync_replicas:\n",
        "      # If sync_replicas is enabled, the averaging will be done in the chief\n",
        "      # queue runner.\n",
        "      optimizer = tf.train.SyncReplicasOptimizer(\n",
        "        opt=optimizer,\n",
        "        replicas_to_aggregate=FLAGS.replicas_to_aggregate,\n",
        "        variable_averages=variable_averages,\n",
        "        variables_to_average=moving_average_variables,\n",
        "        replica_id=tf.constant(FLAGS.task, tf.int32, shape=()),\n",
        "        total_num_replicas=FLAGS.worker_replicas)\n",
        "    elif FLAGS.moving_average_decay:\n",
        "      # Update ops executed locally by trainer.\n",
        "      update_ops.append(variable_averages.apply(moving_average_variables))\n",
        "\n",
        "    # Variables to train.\n",
        "    variables_to_train = _get_variables_to_train()\n",
        "\n",
        "    #  and returns a train_tensor and summary_op\n",
        "    total_loss, clones_gradients = model_deploy.optimize_clones(\n",
        "      clones,\n",
        "      optimizer,\n",
        "      var_list=variables_to_train)\n",
        "    # Add total_loss to summary.\n",
        "    summaries.add(tf.summary.scalar('total_loss', total_loss))\n",
        "\n",
        "    # Create gradient updates.\n",
        "    grad_updates = optimizer.apply_gradients(clones_gradients,\n",
        "                                             global_step=global_step)\n",
        "    update_ops.append(grad_updates)\n",
        "\n",
        "    update_op = tf.group(*update_ops)\n",
        "    train_tensor = control_flow_ops.with_dependencies([update_op], total_loss,\n",
        "                                                      name='train_op')\n",
        "\n",
        "    # Add the summaries from the first clone. These contain the summaries\n",
        "    # created by model_fn and either optimize_clones() or _gather_clone_loss().\n",
        "    summaries |= set(tf.get_collection(tf.GraphKeys.SUMMARIES,\n",
        "                                       first_clone_scope))\n",
        "\n",
        "    # Merge all summaries together.\n",
        "    summary_op = tf.summary.merge(list(summaries), name='summary_op')\n",
        "\n",
        "    ###########################\n",
        "    # Kicks off the training. #\n",
        "    ###########################\n",
        "    slim.learning.train(\n",
        "      train_tensor,\n",
        "      logdir=FLAGS.train_dir,\n",
        "      master=FLAGS.master,\n",
        "      is_chief=(FLAGS.task == 0),\n",
        "      init_fn=_get_init_fn(),\n",
        "      summary_op=summary_op,\n",
        "      number_of_steps=FLAGS.max_number_of_steps,\n",
        "      log_every_n_steps=FLAGS.log_every_n_steps,\n",
        "      save_summaries_secs=FLAGS.save_summaries_secs,\n",
        "      save_interval_secs=FLAGS.save_interval_secs,\n",
        "      sync_optimizer=optimizer if FLAGS.sync_replicas else None)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  tf.app.run()"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}