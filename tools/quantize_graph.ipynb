{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "r\"\"\"Transforms a float-trained graph into an equivalent quantized version.\n",
        "An example of command-line usage is:\n",
        "bazel build tensorflow/tools/quantization:quantize_graph \\\n",
        "&& bazel-bin/tensorflow/tools/quantization/quantize_graph \\\n",
        "--input=tensorflow_inception_graph.pb\n",
        "--output_node_names=\"softmax2\" --print_nodes --output=/tmp/quantized_graph.pb \\\n",
        "--mode=eightbit --logtostderr\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import collections\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.core.framework import attr_value_pb2\n",
        "from tensorflow.core.framework import graph_pb2\n",
        "from tensorflow.core.framework import node_def_pb2\n",
        "from tensorflow.python.client import session\n",
        "from tensorflow.python.framework import constant_op\n",
        "from tensorflow.python.framework import dtypes\n",
        "from tensorflow.python.framework import graph_util\n",
        "from tensorflow.python.framework import importer\n",
        "from tensorflow.python.framework import ops\n",
        "from tensorflow.python.framework import tensor_shape\n",
        "from tensorflow.python.framework import tensor_util\n",
        "from tensorflow.python.ops import array_ops\n",
        "from tensorflow.python.platform import app\n",
        "from tensorflow.python.platform import flags as flags_lib\n",
        "from tensorflow.python.platform import gfile\n",
        "\n",
        "flags = flags_lib\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "flags.DEFINE_boolean(\"print_nodes\", False, \"\"\"Lists all nodes in the model.\"\"\")\n",
        "flags.DEFINE_string(\"input\", \"\", \"\"\"TensorFlow 'GraphDef' file to load.\"\"\")\n",
        "flags.DEFINE_string(\"output_node_names\", \"\",\n",
        "                    \"\"\"Output node names, comma separated.\"\"\")\n",
        "flags.DEFINE_string(\"output\", \"\", \"\"\"File to save the output graph to.\"\"\")\n",
        "flags.DEFINE_integer(\"bitdepth\", 8,\n",
        "                     \"\"\"How many bits to quantize the graph to.\"\"\")\n",
        "flags.DEFINE_string(\"mode\", \"round\",\n",
        "                    \"\"\"What transformation to apply (round, quantize,\"\"\"\n",
        "                    \"\"\" eightbit, weights, or weights_rounded).\"\"\")\n",
        "flags.DEFINE_string(\"test_input_dims\", \"1,224,224,3\",\n",
        "                    \"\"\"The size of the input tensor to use when testing a\"\"\"\n",
        "                    \"\"\" graph loaded from a file.\"\"\")\n",
        "flags.DEFINE_boolean(\"strip_redundant_quantization\", True,\n",
        "                     \"\"\"Removes redundant dequantize/quantize pairs.\"\"\")\n",
        "flags.DEFINE_boolean(\"quantized_input\", False,\n",
        "                     \"If true, assume Placeholders are quantized with values \"\n",
        "                     \"covering [--quantized_input_min,--quantized_input_max]. \"\n",
        "                     \"Only supported when --mode=eightbit\")\n",
        "flags.DEFINE_float(\"quantized_input_min\", 0,\n",
        "                   \"The minimum of the actual input range when \"\n",
        "                   \"--quantized_input\")\n",
        "flags.DEFINE_float(\"quantized_input_max\", 1,\n",
        "                   \"The maximum of the actual input range when \"\n",
        "                   \"--quantized_input\")\n",
        "flags.DEFINE_float(\n",
        "    \"quantized_fallback_min\", None,\n",
        "    \"The fallback 'min' value to use for layers which lack min-max \"\n",
        "    \"information. Note: this should be considered a coarse tool just good \"\n",
        "    \"enough for experimentation purposes, since graphs quantized in this way \"\n",
        "    \"would be very inaccurate.\")\n",
        "flags.DEFINE_float(\n",
        "    \"quantized_fallback_max\", None,\n",
        "    \"The fallback 'max' value to use for layers which lack min-max \"\n",
        "    \"information. Note: this should be considered a coarse tool just good \"\n",
        "    \"enough for experimentation purposes, since graphs quantized in this way \"\n",
        "    \"would be very inaccurate.\")\n",
        "\n",
        "\n",
        "def print_input_nodes(current_node, nodes_map, indent, already_visited):\n",
        "  print(\" \" * indent + current_node.op + \":\" + current_node.name)\n",
        "  already_visited[current_node.name] = True\n",
        "  for input_node_name in current_node.input:\n",
        "    if input_node_name in already_visited:\n",
        "      continue\n",
        "    input_node = nodes_map[input_node_name]\n",
        "    print_input_nodes(input_node, nodes_map, indent + 1, already_visited)\n",
        "\n",
        "\n",
        "def create_node(op, name, inputs):\n",
        "  new_node = node_def_pb2.NodeDef()\n",
        "  new_node.op = op\n",
        "  new_node.name = name\n",
        "  for input_name in inputs:\n",
        "    new_node.input.extend([input_name])\n",
        "  return new_node\n",
        "\n",
        "\n",
        "def create_constant_node(name, value, dtype, shape=None):\n",
        "  node = create_node(\"Const\", name, [])\n",
        "  set_attr_dtype(node, \"dtype\", dtype)\n",
        "  set_attr_tensor(node, \"value\", value, dtype, shape)\n",
        "  return node\n",
        "\n",
        "\n",
        "def copy_attr(node, key, attr_value):\n",
        "  try:\n",
        "    node.attr[key].CopyFrom(attr_value)\n",
        "  except KeyError:\n",
        "    pass\n",
        "\n",
        "\n",
        "def set_attr_dtype(node, key, value):\n",
        "  try:\n",
        "    node.attr[key].CopyFrom(\n",
        "        attr_value_pb2.AttrValue(type=value.as_datatype_enum))\n",
        "  except KeyError:\n",
        "    pass\n",
        "\n",
        "\n",
        "def set_attr_shape(node, key, value):\n",
        "  try:\n",
        "    node.attr[key].CopyFrom(\n",
        "        attr_value_pb2.AttrValue(shape=tensor_shape.as_shape(value).as_proto()))\n",
        "  except KeyError:\n",
        "    pass\n",
        "\n",
        "\n",
        "def set_attr_tensor(node, key, value, dtype, shape=None):\n",
        "  try:\n",
        "    node.attr[key].CopyFrom(\n",
        "        attr_value_pb2.AttrValue(tensor=tensor_util.make_tensor_proto(\n",
        "            value, dtype=dtype, shape=shape)))\n",
        "  except KeyError:\n",
        "    pass\n",
        "\n",
        "\n",
        "def set_attr_string(node, key, value):\n",
        "  try:\n",
        "    node.attr[key].CopyFrom(attr_value_pb2.AttrValue(s=value))\n",
        "  except KeyError:\n",
        "    pass\n",
        "\n",
        "\n",
        "def set_attr_int_list(node, key, value):\n",
        "  list_value = attr_value_pb2.AttrValue.ListValue(i=value)\n",
        "  try:\n",
        "    node.attr[key].CopyFrom(attr_value_pb2.AttrValue(list=list_value))\n",
        "  except KeyError:\n",
        "    pass\n",
        "\n",
        "\n",
        "def set_attr_bool(node, key, value):\n",
        "  try:\n",
        "    node.attr[key].CopyFrom(attr_value_pb2.AttrValue(b=value))\n",
        "  except KeyError:\n",
        "    pass\n",
        "\n",
        "\n",
        "def set_attr_int(node, key, value):\n",
        "  try:\n",
        "    node.attr[key].CopyFrom(attr_value_pb2.AttrValue(i=value))\n",
        "  except KeyError:\n",
        "    pass\n",
        "\n",
        "\n",
        "def set_attr_float(node, key, value):\n",
        "  try:\n",
        "    node.attr[key].CopyFrom(attr_value_pb2.AttrValue(f=value))\n",
        "  except KeyError:\n",
        "    pass\n",
        "\n",
        "\n",
        "def node_name_from_input(node_name):\n",
        "  \"\"\"Strips off ports and other decorations to get the underlying node name.\"\"\"\n",
        "  if node_name.startswith(\"^\"):\n",
        "    node_name = node_name[1:]\n",
        "  m = re.search(r\"(.*):\\d+$\", node_name)\n",
        "  if m:\n",
        "    node_name = m.group(1)\n",
        "  return node_name\n",
        "\n",
        "\n",
        "def ensure_tensor_name_has_port(node_name):\n",
        "  \"\"\"Makes sure that a tensor name has :0 if no explicit port exists.\"\"\"\n",
        "  m = re.search(r\"(.*):\\d+$\", node_name)\n",
        "  if m:\n",
        "    name_with_port = node_name\n",
        "  else:\n",
        "    name_with_port = node_name + \":0\"\n",
        "  return name_with_port\n",
        "\n",
        "\n",
        "def unique_node_name_from_input(node_name):\n",
        "  \"\"\"Replaces invalid characters in input names to get a unique node name.\"\"\"\n",
        "  return node_name.replace(\":\", \"__port__\").replace(\"^\", \"__hat__\")\n",
        "\n",
        "\n",
        "def quantize_array(arr, num_buckets):\n",
        "  \"\"\"Quantizes a numpy array.\n",
        "  This function maps each scalar in arr to the center of one of num_buckets\n",
        "  buckets. For instance,\n",
        "  quantize_array([0, 0.3, 0.6, 1], 2) => [0.25, 0.25, 0.75, 0.75]\n",
        "  Args:\n",
        "    arr: The numpy array to quantize.\n",
        "    num_buckets: The number of buckets to map \"var\" to.\n",
        "  Returns:\n",
        "    The quantized numpy array.\n",
        "  Raises:\n",
        "    ValueError: when num_buckets < 1.\n",
        "  \"\"\"\n",
        "  if num_buckets < 1:\n",
        "    raise ValueError(\"num_buckets must be >= 1\")\n",
        "  arr_max = arr.max()\n",
        "  arr_min = arr.min()\n",
        "  if arr_max == arr_min:\n",
        "    return arr\n",
        "  bucket_width = (arr_max - arr_min) / num_buckets\n",
        "  # Map scalars to bucket indices. Take special care of max(arr).\n",
        "  bucket_indices = np.floor((arr - arr_min) / bucket_width)\n",
        "  bucket_indices[bucket_indices == num_buckets] = num_buckets - 1\n",
        "  # Map each scalar to the center of a bucket.\n",
        "  arr = arr_min + bucket_width * (bucket_indices + 0.5)\n",
        "  return arr\n",
        "\n",
        "\n",
        "def quantize_weight_rounded(input_node):\n",
        "  \"\"\"Returns a replacement node for input_node containing bucketed floats.\"\"\"\n",
        "  input_tensor = input_node.attr[\"value\"].tensor\n",
        "  tensor_value = tensor_util.MakeNdarray(input_tensor)\n",
        "  shape = input_tensor.tensor_shape\n",
        "  # Currently, the parameter FLAGS.bitdepth is used to compute the\n",
        "  # number of buckets as 1 << FLAGS.bitdepth, meaning the number of\n",
        "  # buckets can only be a power of 2.\n",
        "  # This could be fixed by introducing a new parameter, num_buckets,\n",
        "  # which would allow for more flexibility in chosing the right model\n",
        "  # size/accuracy tradeoff. But I didn't want to add more parameters\n",
        "  # to this script than absolutely necessary.\n",
        "  num_buckets = 1 << FLAGS.bitdepth\n",
        "  tensor_value_rounded = quantize_array(tensor_value, num_buckets)\n",
        "  tensor_shape_list = tensor_util.TensorShapeProtoToList(shape)\n",
        "  return [\n",
        "      create_constant_node(\n",
        "          input_node.name,\n",
        "          tensor_value_rounded,\n",
        "          dtypes.float32,\n",
        "          shape=tensor_shape_list)\n",
        "  ]\n",
        "\n",
        "\n",
        "def quantize_weight_eightbit(input_node, quantization_mode):\n",
        "  \"\"\"Returns replacement nodes for input_node using the Dequantize op.\"\"\"\n",
        "  base_name = input_node.name + \"_\"\n",
        "  quint8_const_name = base_name + \"quint8_const\"\n",
        "  min_name = base_name + \"min\"\n",
        "  max_name = base_name + \"max\"\n",
        "  float_tensor = tensor_util.MakeNdarray(input_node.attr[\"value\"].tensor)\n",
        "  min_value = np.min(float_tensor.flatten())\n",
        "  max_value = np.max(float_tensor.flatten())\n",
        "  # Make sure that the range includes zero.\n",
        "  if min_value > 0.0:\n",
        "    min_value = 0.0\n",
        "  # min_value == max_value is a tricky case. It can occur for general\n",
        "  # tensors, and of course for scalars. The quantized ops cannot deal\n",
        "  # with this case, so we set max_value to something else.\n",
        "  # It's a tricky question what is the numerically best solution to\n",
        "  # deal with this degeneracy.\n",
        "  # TODO(petewarden): Better use a tolerance than a hard comparison?\n",
        "  if min_value == max_value:\n",
        "    if abs(min_value) < 0.000001:\n",
        "      max_value = min_value + 1.0\n",
        "    elif min_value > 0:\n",
        "      max_value = 2 * min_value\n",
        "    else:\n",
        "      max_value = min_value / 2.0\n",
        "\n",
        "  sess = session.Session()\n",
        "  with sess.as_default():\n",
        "    quantize_op = array_ops.quantize_v2(\n",
        "        float_tensor,\n",
        "        min_value,\n",
        "        max_value,\n",
        "        dtypes.quint8,\n",
        "        mode=quantization_mode)\n",
        "    quint8_tensor = quantize_op[0].eval()\n",
        "  shape = tensor_util.TensorShapeProtoToList(input_node.attr[\"value\"]\n",
        "                                             .tensor.tensor_shape)\n",
        "  quint8_const_node = create_constant_node(\n",
        "      quint8_const_name, quint8_tensor, dtypes.quint8, shape=shape)\n",
        "  min_node = create_constant_node(min_name, min_value, dtypes.float32)\n",
        "  max_node = create_constant_node(max_name, max_value, dtypes.float32)\n",
        "  dequantize_node = create_node(\"Dequantize\", input_node.name,\n",
        "                                [quint8_const_name, min_name, max_name])\n",
        "  set_attr_dtype(dequantize_node, \"T\", dtypes.quint8)\n",
        "  set_attr_string(dequantize_node, \"mode\", quantization_mode)\n",
        "  return [quint8_const_node, min_node, max_node, dequantize_node]\n",
        "\n",
        "\n",
        "EightbitizeRecursionState = collections.namedtuple(\n",
        "    \"EightbitizeRecursionState\",\n",
        "    [\"already_visited\", \"output_node_stack\", \"merged_with_fake_quant\"])\n",
        "\n",
        "\n",
        "class GraphRewriter(object):\n",
        "  \"\"\"Takes a float graph, and rewrites it in quantized form.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               input_graph,\n",
        "               mode,\n",
        "               quantized_input_range,\n",
        "               fallback_quantization_range=None):\n",
        "    \"\"\"Sets up the class to rewrite a float graph.\n",
        "    Args:\n",
        "      input_graph: A float graph to transform.\n",
        "      mode: A string controlling how quantization is performed -\n",
        "        round, quantize, eightbit, or weights.\n",
        "      quantized_input_range: if set, assume the input is\n",
        "        quantized and represents the range\n",
        "        [quantized_input_range[0], quantized_input_range[1]]\n",
        "      fallback_quantization_range: if set, then for nodes where the quantization\n",
        "        range can't be inferred from the graph, use the range\n",
        "        [fallback_quantization_range[0], fallback_quantization_range[1]) instead\n",
        "        of using a RequantizationRange node in the graph.\n",
        "    Raises:\n",
        "      ValueError: Two nodes with the same name were found in the graph.\n",
        "    \"\"\"\n",
        "    self.input_graph = input_graph\n",
        "    self.nodes_map = self.create_nodes_map(input_graph)\n",
        "    self.output_graph = None\n",
        "    self.mode = mode\n",
        "    self.final_node_renames = {}\n",
        "    if quantized_input_range:\n",
        "      self.input_range = (quantized_input_range[0], quantized_input_range[1])\n",
        "      if self.input_range[0] >= self.input_range[1]:\n",
        "        raise ValueError(\"Invalid quantized_input_range: [%s,%s]\" %\n",
        "                         self.input_range)\n",
        "      if self.mode != \"eightbit\":\n",
        "        raise ValueError(\n",
        "            \"quantized_input_range can only be specified in eightbit mode\")\n",
        "    else:\n",
        "      self.input_range = None\n",
        "\n",
        "    if fallback_quantization_range:\n",
        "      self.fallback_quantization_range = [\n",
        "          fallback_quantization_range[0], fallback_quantization_range[1]\n",
        "      ]\n",
        "      if (self.fallback_quantization_range[0] >=\n",
        "          self.fallback_quantization_range[1]):\n",
        "        raise ValueError(\"Invalid fallback_quantization_range: [%s,%s]\" %\n",
        "                         self.fallback_quantization_range)\n",
        "      if self.mode != \"eightbit\":\n",
        "        raise ValueError(\"fallback_quantization_range can only be \"\n",
        "                         \"specified in eightbit mode\")\n",
        "    else:\n",
        "      self.fallback_quantization_range = None\n",
        "\n",
        "    # Data that is valid only during the recursive call to rewrite the graph.\n",
        "    self.state = None\n",
        "\n",
        "  def create_nodes_map(self, graph):\n",
        "    \"\"\"Builds a mapping of node names to their defs from the graph.\"\"\"\n",
        "    nodes_map = {}\n",
        "    for node in graph.node:\n",
        "      if node.name not in nodes_map.keys():\n",
        "        nodes_map[node.name] = node\n",
        "      else:\n",
        "        raise ValueError(\"Duplicate node names detected.\")\n",
        "    return nodes_map\n",
        "\n",
        "  def rewrite(self, output_node_names):\n",
        "    \"\"\"Triggers rewriting of the float graph.\n",
        "    Args:\n",
        "      output_node_names: A list of names of the nodes that produce the final\n",
        "        results.\n",
        "    Returns:\n",
        "      A quantized version of the float graph.\n",
        "    \"\"\"\n",
        "    self.output_graph = graph_pb2.GraphDef()\n",
        "    output_nodes = [\n",
        "        self.nodes_map[output_node_name]\n",
        "        for output_node_name in output_node_names\n",
        "    ]\n",
        "    if self.mode == \"round\":\n",
        "      self.already_visited = {}\n",
        "      for output_node in output_nodes:\n",
        "        self.round_nodes_recursively(output_node)\n",
        "    elif self.mode == \"quantize\":\n",
        "      self.already_visited = {}\n",
        "      self.already_quantized = {}\n",
        "      for output_node in output_nodes:\n",
        "        self.quantize_nodes_recursively(output_node)\n",
        "    elif self.mode == \"eightbit\":\n",
        "      self.set_input_graph(graph_util.remove_training_nodes(self.input_graph))\n",
        "      output_nodes = [\n",
        "          self.nodes_map[output_node_name]\n",
        "          for output_node_name in output_node_names\n",
        "      ]\n",
        "\n",
        "      self.state = EightbitizeRecursionState(\n",
        "          already_visited={}, output_node_stack=[], merged_with_fake_quant={})\n",
        "      for output_node in output_nodes:\n",
        "        self.eightbitize_nodes_recursively(output_node)\n",
        "      self.state = None\n",
        "      if self.input_range:\n",
        "        self.add_output_graph_node(\n",
        "            create_constant_node(\"quantized_input_min_value\", self.input_range[\n",
        "                0], dtypes.float32, []))\n",
        "        self.add_output_graph_node(\n",
        "            create_constant_node(\"quantized_input_max_value\", self.input_range[\n",
        "                1], dtypes.float32, []))\n",
        "      if self.fallback_quantization_range:\n",
        "        self.add_output_graph_node(\n",
        "            create_constant_node(\"fallback_quantization_min_value\",\n",
        "                                 self.fallback_quantization_range[0],\n",
        "                                 dtypes.float32, []))\n",
        "        self.add_output_graph_node(\n",
        "            create_constant_node(\"fallback_quantization_max_value\",\n",
        "                                 self.fallback_quantization_range[1],\n",
        "                                 dtypes.float32, []))\n",
        "      if FLAGS.strip_redundant_quantization:\n",
        "        self.output_graph = self.remove_redundant_quantization(\n",
        "            self.output_graph)\n",
        "        self.remove_dead_nodes(output_node_names)\n",
        "      self.apply_final_node_renames()\n",
        "    elif self.mode == \"weights\":\n",
        "      self.output_graph = self.quantize_weights(self.input_graph,\n",
        "                                                b\"MIN_COMBINED\")\n",
        "      self.remove_dead_nodes(output_node_names)\n",
        "    elif self.mode == \"weights_rounded\":\n",
        "      self.output_graph = self.quantize_weights(self.input_graph, self.mode)\n",
        "      self.remove_dead_nodes(output_node_names)\n",
        "    else:\n",
        "      print(\"Bad mode - \" + self.mode + \".\")\n",
        "    return self.output_graph\n",
        "\n",
        "  def round_nodes_recursively(self, current_node):\n",
        "    \"\"\"The entry point for simple rounding quantization.\"\"\"\n",
        "    if self.already_visited[current_node.name]:\n",
        "      return\n",
        "    self.already_visited[current_node.name] = True\n",
        "    for input_node_name in current_node.input:\n",
        "      input_node_name = node_name_from_input(input_node_name)\n",
        "      input_node = self.nodes_map[input_node_name]\n",
        "      self.round_nodes_recursively(input_node)\n",
        "    nodes_to_quantize = [\"Conv2D\", \"BiasAdd\", \"MatMul\"]\n",
        "    if any(current_node.op in s for s in nodes_to_quantize):\n",
        "      new_node = node_def_pb2.NodeDef()\n",
        "      new_node.CopyFrom(current_node)\n",
        "      new_node.name = current_node.name + \"_original\"\n",
        "      self.add_output_graph_node(new_node)\n",
        "      levels = 1 << FLAGS.bitdepth\n",
        "      constant_name = current_node.name + \"_round_depth\"\n",
        "      constant_tensor = constant_op.constant(\n",
        "          levels, dtype=dtypes.int32, name=constant_name)\n",
        "      constant_node = constant_tensor.op.node_def\n",
        "      self.add_output_graph_node(constant_node)\n",
        "      quantize_node = node_def_pb2.NodeDef()\n",
        "      quantize_node.op = \"RoundToSteps\"\n",
        "      quantize_node.name = current_node.name\n",
        "      quantize_node.input.extend([current_node.name + \"_original\"])\n",
        "      quantize_node.input.extend([constant_node.name])\n",
        "      self.add_output_graph_node(quantize_node)\n",
        "    else:\n",
        "      new_node = node_def_pb2.NodeDef()\n",
        "      new_node.CopyFrom(current_node)\n",
        "      self.add_output_graph_node(new_node)\n",
        "\n",
        "  def quantize_nodes_recursively(self, current_node):\n",
        "    \"\"\"The entry point for quantizing nodes to eight bit and back.\"\"\"\n",
        "    if self.already_visited[current_node.name]:\n",
        "      return\n",
        "    self.already_visited[current_node.name] = True\n",
        "    for input_node_name in current_node.input:\n",
        "      input_node_name = node_name_from_input(input_node_name)\n",
        "      input_node = self.nodes_map[input_node_name]\n",
        "      self.quantize_nodes_recursively(input_node)\n",
        "    nodes_to_quantize = [\"Conv2D\", \"BiasAdd\", \"MatMul\"]\n",
        "    if any(current_node.op in s for s in nodes_to_quantize):\n",
        "      for input_name in current_node.input:\n",
        "        input_name = node_name_from_input(input_name)\n",
        "        input_node = self.nodes_map[input_name]\n",
        "        self.quantize_node(input_node)\n",
        "      self.quantize_node(current_node)\n",
        "    else:\n",
        "      new_node = node_def_pb2.NodeDef()\n",
        "      new_node.CopyFrom(current_node)\n",
        "      self.add_output_graph_node(new_node)\n",
        "\n",
        "  def quantize_node(self, input_node):\n",
        "    \"\"\"Handles quantizing a single node.\"\"\"\n",
        "    input_name = input_node.name\n",
        "    if input_name in self.already_quantized:\n",
        "      return\n",
        "    self.already_quantized[input_name] = True\n",
        "    original_input_name = input_name + \"_original\"\n",
        "    reshape_name = input_name + \"_reshape\"\n",
        "    reshape_dims_name = input_name + \"_reshape_dims\"\n",
        "    max_name = input_name + \"_max\"\n",
        "    min_name = input_name + \"_min\"\n",
        "    dims_name = input_name + \"_dims\"\n",
        "    quantize_name = input_name + \"_quantize\"\n",
        "    dequantize_name = input_name\n",
        "    original_input_node = node_def_pb2.NodeDef()\n",
        "    original_input_node.CopyFrom(input_node)\n",
        "    original_input_node.name = original_input_name\n",
        "    self.add_output_graph_node(original_input_node)\n",
        "    reshape_dims_node = create_constant_node(reshape_dims_name, -1,\n",
        "                                             dtypes.int32, [1])\n",
        "    self.add_output_graph_node(reshape_dims_node)\n",
        "    reshape_node = create_node(\"Reshape\", reshape_name,\n",
        "                               [original_input_name, reshape_dims_name])\n",
        "    set_attr_dtype(reshape_node, \"T\", dtypes.float32)\n",
        "    self.add_output_graph_node(reshape_node)\n",
        "    dims_node = create_constant_node(dims_name, 0, dtypes.int32, [1])\n",
        "    self.add_output_graph_node(dims_node)\n",
        "    max_node = create_node(\"Max\", max_name, [reshape_name, dims_name])\n",
        "    set_attr_dtype(max_node, \"T\", dtypes.float32)\n",
        "    set_attr_bool(max_node, \"keep_dims\", False)\n",
        "    self.add_output_graph_node(max_node)\n",
        "    min_node = create_node(\"Min\", min_name, [reshape_name, dims_name])\n",
        "    set_attr_dtype(min_node, \"T\", dtypes.float32)\n",
        "    set_attr_bool(min_node, \"keep_dims\", False)\n",
        "    self.add_output_graph_node(min_node)\n",
        "    quantize_node = create_node(\"Quantize\", quantize_name,\n",
        "                                [original_input_name, min_name, max_name])\n",
        "    set_attr_dtype(quantize_node, \"T\", dtypes.quint8)\n",
        "    set_attr_string(quantize_node, \"mode\", b\"MIN_FIRST\")\n",
        "    self.add_output_graph_node(quantize_node)\n",
        "    dequantize_node = create_node(\"Dequantize\", dequantize_name,\n",
        "                                  [quantize_name, min_name, max_name])\n",
        "    set_attr_dtype(dequantize_node, \"T\", dtypes.quint8)\n",
        "    set_attr_string(dequantize_node, \"mode\", b\"MIN_FIRST\")\n",
        "    self.add_output_graph_node(dequantize_node)\n",
        "\n",
        "  def should_merge_with_fake_quant_node(self):\n",
        "    \"\"\"Should the current node merge with self.state.output_node_stack[-1]?\"\"\"\n",
        "    if not self.state.output_node_stack:\n",
        "      return False\n",
        "    top = self.state.output_node_stack[-1]\n",
        "    return top[1] == 0 and top[0].op in [\"FakeQuantWithMinMaxVars\"]\n",
        "\n",
        "  def should_quantize_const(self, node):\n",
        "    if not self.state.output_node_stack:\n",
        "      return False\n",
        "    top = self.state.output_node_stack[-1]\n",
        "    if not top[2]:\n",
        "      return False\n",
        "    dtype = dtypes.as_dtype(node.attr[\"dtype\"].type)\n",
        "    assert dtype == dtypes.float32, (\n",
        "        \"Failed to quantized constant %s of type %s\" % (node.name, dtype))\n",
        "    return True\n",
        "\n",
        "  def eightbitize_nodes_recursively(self, current_node):\n",
        "    \"\"\"The entry point for transforming a graph into full eight bit.\"\"\"\n",
        "    if current_node.name in self.state.already_visited:\n",
        "      if (self.should_merge_with_fake_quant_node() or\n",
        "          current_node.name in self.state.merged_with_fake_quant):\n",
        "        raise ValueError(\"Unsupported graph structure: output of node %s \"\n",
        "                         \"is processed by a FakeQuant* node and should have \"\n",
        "                         \"no other outputs.\", current_node.name)\n",
        "      return\n",
        "    self.state.already_visited[current_node.name] = True\n",
        "\n",
        "    for i, input_node_name in enumerate(current_node.input):\n",
        "      quantize_input = False\n",
        "      if current_node.op in (\"MatMul\", \"Conv2D\", \"BiasAdd\", \"MaxPool\",\n",
        "                             \"AvgPool\", \"Relu\", \"Relu6\",\n",
        "                             \"BatchNormWithGlobalNormalization\"):\n",
        "        quantize_input = True\n",
        "      elif current_node.op == \"Concat\" and i > 0:\n",
        "        quantize_input = (\n",
        "            dtypes.as_dtype(current_node.attr[\"T\"].type) == dtypes.float32)\n",
        "      elif current_node.op == \"Reshape\" and i == 0:\n",
        "        quantize_input = (\n",
        "            dtypes.as_dtype(current_node.attr[\"T\"].type) == dtypes.float32)\n",
        "\n",
        "      self.state.output_node_stack.append((current_node, i, quantize_input))\n",
        "\n",
        "      input_node_name = node_name_from_input(input_node_name)\n",
        "      input_node = self.nodes_map[input_node_name]\n",
        "      self.eightbitize_nodes_recursively(input_node)\n",
        "\n",
        "      self.state.output_node_stack.pop()\n",
        "\n",
        "    if current_node.op == \"MatMul\":\n",
        "      self.eightbitize_mat_mul_node(current_node)\n",
        "    elif current_node.op == \"Conv2D\":\n",
        "      self.eightbitize_conv_node(current_node)\n",
        "    elif current_node.op == \"BiasAdd\":\n",
        "      self.eightbitize_bias_add_node(current_node)\n",
        "    elif current_node.op == \"MaxPool\" or current_node.op == \"AvgPool\":\n",
        "      self.eightbitize_single_input_tensor_node(current_node,\n",
        "                                                self.add_pool_function)\n",
        "    elif current_node.op == \"Relu\" or current_node.op == \"Relu6\":\n",
        "      self.eightbitize_single_input_tensor_node(current_node,\n",
        "                                                self.add_relu_function)\n",
        "    elif (current_node.op == \"Concat\" and\n",
        "          dtypes.as_dtype(current_node.attr[\"T\"].type) == dtypes.float32):\n",
        "      self.eightbitize_concat_node(current_node)\n",
        "    elif current_node.op == \"BatchNormWithGlobalNormalization\":\n",
        "      self.eightbitize_batch_norm_node(current_node)\n",
        "    elif (current_node.op == \"Reshape\" and\n",
        "          dtypes.as_dtype(current_node.attr[\"T\"].type) == dtypes.float32):\n",
        "      self.eightbitize_reshape_node(current_node)\n",
        "    elif (self.input_range and\n",
        "          current_node.op in (\"Placeholder\", \"PlaceholderV2\")):\n",
        "      self.eightbitize_placeholder_node(current_node)\n",
        "    elif current_node.op == \"FakeQuantWithMinMaxVars\":\n",
        "      # It will have been merged into the underlying node.\n",
        "      pass\n",
        "    elif current_node.op == \"Const\":\n",
        "      if self.should_quantize_const(current_node):\n",
        "        for n in quantize_weight_eightbit(current_node, b\"MIN_FIRST\"):\n",
        "          self.add_output_graph_node(n)\n",
        "      else:\n",
        "        new_node = node_def_pb2.NodeDef()\n",
        "        new_node.CopyFrom(current_node)\n",
        "        self.add_output_graph_node(new_node)\n",
        "\n",
        "    ###################################################################\n",
        "    # Note: if more cases are added here, you may need to update the op\n",
        "    # name lists in the loop over children at the start of the function.\n",
        "    ###################################################################\n",
        "    else:\n",
        "      new_node = node_def_pb2.NodeDef()\n",
        "      new_node.CopyFrom(current_node)\n",
        "      self.add_output_graph_node(new_node)\n",
        "\n",
        "    if (self.should_merge_with_fake_quant_node() and\n",
        "        current_node.name not in self.state.merged_with_fake_quant):\n",
        "      raise ValueError(\n",
        "          \"FakeQuant* node %s failed to merge with node %s of type %s\" %\n",
        "          (self.state.output_node_stack[-1][0], current_node.name,\n",
        "           current_node.op))\n",
        "\n",
        "  def add_eightbit_prologue_nodes(self, original_node):\n",
        "    \"\"\"Adds input conversion nodes to handle quantizing the underlying node.\"\"\"\n",
        "    namespace_prefix = original_node.name + \"_eightbit\"\n",
        "    reshape_dims_name, reduction_dims_name = self.add_common_quantization_nodes(\n",
        "        namespace_prefix)\n",
        "    input_names = []\n",
        "    min_max_names = []\n",
        "    for original_input_name in original_node.input:\n",
        "      quantize_input_name, min_input_name, max_input_name = (\n",
        "          self.eightbitize_input_to_node(namespace_prefix, original_input_name,\n",
        "                                         reshape_dims_name,\n",
        "                                         reduction_dims_name))\n",
        "      input_names.append(quantize_input_name)\n",
        "      min_max_names.append(min_input_name)\n",
        "      min_max_names.append(max_input_name)\n",
        "    all_input_names = []\n",
        "    all_input_names.extend(input_names)\n",
        "    all_input_names.extend(min_max_names)\n",
        "    return all_input_names\n",
        "\n",
        "  def add_common_quantization_nodes(self, namespace_prefix):\n",
        "    \"\"\"Builds constant nodes needed for quantization of inputs.\"\"\"\n",
        "    reshape_dims_name = namespace_prefix + \"_reshape_dims\"\n",
        "    reduction_dims_name = namespace_prefix + \"_reduction_dims\"\n",
        "\n",
        "    reshape_dims_node = create_constant_node(reshape_dims_name, -1,\n",
        "                                             dtypes.int32, [1])\n",
        "    self.add_output_graph_node(reshape_dims_node)\n",
        "    reduction_dims_node = create_constant_node(reduction_dims_name, 0,\n",
        "                                               dtypes.int32, [1])\n",
        "    self.add_output_graph_node(reduction_dims_node)\n",
        "    return reshape_dims_name, reduction_dims_name\n",
        "\n",
        "  def eightbitize_input_to_node(self, namespace_prefix, original_input_name,\n",
        "                                reshape_dims_name, reduction_dims_name):\n",
        "    \"\"\"Takes one float input to an op, and converts it to quantized form.\"\"\"\n",
        "    unique_input_name = unique_node_name_from_input(original_input_name)\n",
        "    reshape_input_name = namespace_prefix + \"_reshape_\" + unique_input_name\n",
        "    min_input_name = namespace_prefix + \"_min_\" + unique_input_name\n",
        "    max_input_name = namespace_prefix + \"_max_\" + unique_input_name\n",
        "    quantize_input_name = namespace_prefix + \"_quantize_\" + unique_input_name\n",
        "    reshape_input_node = create_node(\"Reshape\", reshape_input_name,\n",
        "                                     [original_input_name, reshape_dims_name])\n",
        "    set_attr_dtype(reshape_input_node, \"T\", dtypes.float32)\n",
        "    self.add_output_graph_node(reshape_input_node)\n",
        "    min_input_node = create_node(\"Min\", min_input_name,\n",
        "                                 [reshape_input_name, reduction_dims_name])\n",
        "    set_attr_dtype(min_input_node, \"T\", dtypes.float32)\n",
        "    set_attr_bool(min_input_node, \"keep_dims\", False)\n",
        "    self.add_output_graph_node(min_input_node)\n",
        "    max_input_node = create_node(\"Max\", max_input_name,\n",
        "                                 [reshape_input_name, reduction_dims_name])\n",
        "    set_attr_dtype(max_input_node, \"T\", dtypes.float32)\n",
        "    set_attr_bool(max_input_node, \"keep_dims\", False)\n",
        "    self.add_output_graph_node(max_input_node)\n",
        "    quantize_input_node = create_node(\n",
        "        \"QuantizeV2\", quantize_input_name,\n",
        "        [original_input_name, min_input_name, max_input_name])\n",
        "    set_attr_dtype(quantize_input_node, \"T\", dtypes.quint8)\n",
        "    set_attr_string(quantize_input_node, \"mode\", b\"MIN_FIRST\")\n",
        "    self.add_output_graph_node(quantize_input_node)\n",
        "    min_output_name = quantize_input_name + \":1\"\n",
        "    max_output_name = quantize_input_name + \":2\"\n",
        "    return quantize_input_name, min_output_name, max_output_name\n",
        "\n",
        "  def add_quantize_down_nodes(self, original_node, quantized_output_name):\n",
        "    quantized_outputs = [\n",
        "        quantized_output_name, quantized_output_name + \":1\",\n",
        "        quantized_output_name + \":2\"\n",
        "    ]\n",
        "    min_max_inputs = None\n",
        "    if self.should_merge_with_fake_quant_node():\n",
        "      # Use the inputs to the FakeQuantWithMinMaxVars node as the inputs to\n",
        "      # Requantize.\n",
        "      fake_quant_node = self.state.output_node_stack[-1][0]\n",
        "      min_max_inputs = [fake_quant_node.input[1], fake_quant_node.input[2]]\n",
        "      assert original_node.name not in self.state.merged_with_fake_quant\n",
        "      self.state.merged_with_fake_quant[original_node.name] = True\n",
        "    elif self.fallback_quantization_range:\n",
        "      min_max_inputs = [\n",
        "          \"fallback_quantization_min_value:0\",\n",
        "          \"fallback_quantization_max_value:0\"\n",
        "      ]\n",
        "    else:\n",
        "      # Add a RequantizationRange node for finding the min and max values.\n",
        "      requant_range_node = create_node(\n",
        "          \"RequantizationRange\", original_node.name + \"_eightbit_requant_range\",\n",
        "          quantized_outputs)\n",
        "      set_attr_dtype(requant_range_node, \"Tinput\", dtypes.qint32)\n",
        "      self.add_output_graph_node(requant_range_node)\n",
        "      min_max_inputs = [\n",
        "          requant_range_node.name + \":0\", requant_range_node.name + \":1\"\n",
        "      ]\n",
        "    requantize_node = create_node(\"Requantize\",\n",
        "                                  original_node.name + \"_eightbit_requantize\",\n",
        "                                  quantized_outputs + min_max_inputs)\n",
        "    set_attr_dtype(requantize_node, \"Tinput\", dtypes.qint32)\n",
        "    set_attr_dtype(requantize_node, \"out_type\", dtypes.quint8)\n",
        "    self.add_output_graph_node(requantize_node)\n",
        "    return requantize_node.name\n",
        "\n",
        "  def add_dequantize_result_node(self,\n",
        "                                 quantized_output_name,\n",
        "                                 original_node_name,\n",
        "                                 min_tensor_index=1):\n",
        "    min_max_inputs = [\n",
        "        \"%s:%s\" % (quantized_output_name, min_tensor_index),\n",
        "        \"%s:%s\" % (quantized_output_name, (min_tensor_index + 1))\n",
        "    ]\n",
        "    dequantize_name = original_node_name\n",
        "    if self.should_merge_with_fake_quant_node():\n",
        "      fake_quant_node = self.state.output_node_stack[-1][0]\n",
        "      if original_node_name not in self.state.merged_with_fake_quant:\n",
        "        min_max_inputs = [fake_quant_node.input[1], fake_quant_node.input[2]]\n",
        "        self.state.merged_with_fake_quant[original_node_name] = True\n",
        "      dequantize_name = fake_quant_node.name\n",
        "\n",
        "    dequantize_node = create_node(\n",
        "        \"Dequantize\", dequantize_name,\n",
        "        [quantized_output_name, min_max_inputs[0], min_max_inputs[1]])\n",
        "    set_attr_dtype(dequantize_node, \"T\", dtypes.quint8)\n",
        "    set_attr_string(dequantize_node, \"mode\", b\"MIN_FIRST\")\n",
        "    self.add_output_graph_node(dequantize_node)\n",
        "\n",
        "  def eightbitize_mat_mul_node(self, original_node):\n",
        "    \"\"\"Replaces a MatMul node with the eight bit equivalent sub-graph.\"\"\"\n",
        "    quantized_mat_mul_name = original_node.name + \"_eightbit_quantized_mat_mul\"\n",
        "    all_input_names = self.add_eightbit_prologue_nodes(original_node)\n",
        "    quantized_mat_mul_node = create_node(\"QuantizedMatMul\",\n",
        "                                         quantized_mat_mul_name,\n",
        "                                         all_input_names)\n",
        "    set_attr_dtype(quantized_mat_mul_node, \"T1\", dtypes.quint8)\n",
        "    set_attr_dtype(quantized_mat_mul_node, \"T2\", dtypes.quint8)\n",
        "    set_attr_dtype(quantized_mat_mul_node, \"Toutput\", dtypes.qint32)\n",
        "    copy_attr(quantized_mat_mul_node, \"transpose_a\",\n",
        "              original_node.attr[\"transpose_a\"])\n",
        "    copy_attr(quantized_mat_mul_node, \"transpose_b\",\n",
        "              original_node.attr[\"transpose_b\"])\n",
        "    self.add_output_graph_node(quantized_mat_mul_node)\n",
        "    quantize_down_name = self.add_quantize_down_nodes(original_node,\n",
        "                                                      quantized_mat_mul_name)\n",
        "    self.add_dequantize_result_node(quantize_down_name, original_node.name)\n",
        "\n",
        "  def eightbitize_conv_node(self, original_node):\n",
        "    \"\"\"Replaces a Conv2D node with the eight bit equivalent sub-graph.\"\"\"\n",
        "    all_input_names = self.add_eightbit_prologue_nodes(original_node)\n",
        "    quantized_conv_name = original_node.name + \"_eightbit_quantized_conv\"\n",
        "    quantized_conv_node = create_node(\"QuantizedConv2D\", quantized_conv_name,\n",
        "                                      all_input_names)\n",
        "    copy_attr(quantized_conv_node, \"strides\", original_node.attr[\"strides\"])\n",
        "    copy_attr(quantized_conv_node, \"padding\", original_node.attr[\"padding\"])\n",
        "    set_attr_dtype(quantized_conv_node, \"Tinput\", dtypes.quint8)\n",
        "    set_attr_dtype(quantized_conv_node, \"Tfilter\", dtypes.quint8)\n",
        "    set_attr_dtype(quantized_conv_node, \"out_type\", dtypes.qint32)\n",
        "    self.add_output_graph_node(quantized_conv_node)\n",
        "    quantize_down_name = self.add_quantize_down_nodes(original_node,\n",
        "                                                      quantized_conv_name)\n",
        "    self.add_dequantize_result_node(quantize_down_name, original_node.name)\n",
        "\n",
        "  def eightbitize_bias_add_node(self, original_node):\n",
        "    \"\"\"Replaces a BiasAdd node with the eight bit equivalent sub-graph.\"\"\"\n",
        "    quantized_bias_add_name = (\n",
        "        original_node.name + \"_eightbit_quantized_bias_add\")\n",
        "    all_input_names = self.add_eightbit_prologue_nodes(original_node)\n",
        "    quantized_bias_add_node = create_node(\"QuantizedBiasAdd\",\n",
        "                                          quantized_bias_add_name,\n",
        "                                          all_input_names)\n",
        "    set_attr_dtype(quantized_bias_add_node, \"T1\", dtypes.quint8)\n",
        "    set_attr_dtype(quantized_bias_add_node, \"T2\", dtypes.quint8)\n",
        "    set_attr_dtype(quantized_bias_add_node, \"out_type\", dtypes.qint32)\n",
        "    self.add_output_graph_node(quantized_bias_add_node)\n",
        "    quantize_down_name = self.add_quantize_down_nodes(original_node,\n",
        "                                                      quantized_bias_add_name)\n",
        "    self.add_dequantize_result_node(quantize_down_name, original_node.name)\n",
        "\n",
        "  def eightbitize_single_input_tensor_node(self, original_node,\n",
        "                                           add_op_function):\n",
        "    \"\"\"Replaces a single-tensor node with the eight bit equivalent sub-graph.\n",
        "    Converts a node like this:\n",
        "       Shape(f)   Input(f)\n",
        "         |          |\n",
        "         +--------v v\n",
        "                Operation\n",
        "                    |\n",
        "                    v\n",
        "                   (f)\n",
        "     Into a quantized equivalent:\n",
        "                    Input(f)              ReshapeDims\n",
        "                       +------v v-------------+\n",
        "                       |    Reshape\n",
        "                       |      |\n",
        "                       |      |          ReductionDims\n",
        "                       |      +-----+         |\n",
        "                       |      | +---c---------+\n",
        "                       |      v v   v v-------+\n",
        "                       |      Min   Max\n",
        "                       |  +----+      |\n",
        "                       v  v  v--------+\n",
        "                      Quantize\n",
        "                          |\n",
        "                          v\n",
        "                   QuantizedOperation\n",
        "                      |   |   |\n",
        "                      v   v   v\n",
        "                      Dequantize\n",
        "                          |\n",
        "                          v\n",
        "                         (f)\n",
        "    Args:\n",
        "      original_node: Float node to be converted.\n",
        "      add_op_function: Function to create the actual node.\n",
        "    Returns:\n",
        "      Subgraph representing the quantized version of the original node.\n",
        "    \"\"\"\n",
        "    quantized_op_name = original_node.name + \"_eightbit_quantized\"\n",
        "    quantized_op_type = \"Quantized\" + original_node.op\n",
        "    all_input_names = self.add_eightbit_prologue_nodes(original_node)\n",
        "    quantized_op_node = create_node(quantized_op_type, quantized_op_name,\n",
        "                                    all_input_names)\n",
        "    add_op_function(original_node, quantized_op_node)\n",
        "    self.add_output_graph_node(quantized_op_node)\n",
        "    self.add_dequantize_result_node(quantized_op_name, original_node.name)\n",
        "\n",
        "  def add_pool_function(self, original_node, quantized_op_node):\n",
        "    set_attr_dtype(quantized_op_node, \"T\", dtypes.quint8)\n",
        "    copy_attr(quantized_op_node, \"ksize\", original_node.attr[\"ksize\"])\n",
        "    copy_attr(quantized_op_node, \"strides\", original_node.attr[\"strides\"])\n",
        "    copy_attr(quantized_op_node, \"padding\", original_node.attr[\"padding\"])\n",
        "\n",
        "  def add_relu_function(self, unused_arg_node, quantized_op_node):\n",
        "    set_attr_dtype(quantized_op_node, \"Tinput\", dtypes.quint8)\n",
        "\n",
        "  def eightbitize_concat_node(self, original_node):\n",
        "    \"\"\"Replaces a Concat node with the eight bit equivalent sub-graph.\n",
        "    Converts a node like this:\n",
        "       Shape(f)   Input0(f)   Input1(f)\n",
        "         |          |            |\n",
        "         +--------v v v----------+\n",
        "                  Concat\n",
        "                    |\n",
        "                    v\n",
        "                   (f)\n",
        "     Into a quantized equivalent:\n",
        "       Shape(f)     Input0(f)             ReshapeDims                  Input1(f)\n",
        "         |             +------v v--------------+------------------v v------+\n",
        "         |             |    Reshape                             Reshape    |\n",
        "         |             |      |                                     |      |\n",
        "         |             |      |           ReductionDims             |      |\n",
        "         |             |      +------+         |           +--------+      |\n",
        "         |             |      |  +---c---------+-----------c-----+  |      |\n",
        "         |             |      +v v   v v-------+---------v v     v v+      |\n",
        "         |             |       Min   Max                 Min     Max       |\n",
        "         |             |  +----+      |                   |       +-----+  |\n",
        "         |             v  v  v--------+                   +----------v  v  v\n",
        "         |            Quantize                                       Quantize\n",
        "         |                +------------------+   +----------------------+\n",
        "         +-------------------------------+   |   |\n",
        "                                         v   v   v\n",
        "                                      QuantizedConcat\n",
        "                                         |   |   |\n",
        "                                         v   v   v\n",
        "                                        Dequantize\n",
        "                                             |\n",
        "                                             v\n",
        "                                            (f)\n",
        "    Args:\n",
        "      original_node: Float node to be converted.\n",
        "    Returns:\n",
        "      Subgraph representing the quantized version of the original node.\n",
        "    \"\"\"\n",
        "    namespace_prefix = original_node.name + \"_eightbit\"\n",
        "    quantized_concat_name = namespace_prefix + \"_quantized_concat\"\n",
        "    reshape_dims_name, reduction_dims_name = self.add_common_quantization_nodes(\n",
        "        namespace_prefix)\n",
        "    shape_input_name = original_node.input[0]\n",
        "    original_inputs = original_node.input[1:]\n",
        "    input_names = []\n",
        "    min_names = []\n",
        "    max_names = []\n",
        "    for original_input_name in original_inputs:\n",
        "      quantize_input_name, min_input_name, max_input_name = (\n",
        "          self.eightbitize_input_to_node(namespace_prefix, original_input_name,\n",
        "                                         reshape_dims_name,\n",
        "                                         reduction_dims_name))\n",
        "      input_names.append(quantize_input_name)\n",
        "      min_names.append(min_input_name)\n",
        "      max_names.append(max_input_name)\n",
        "    all_input_names = [shape_input_name]\n",
        "    all_input_names.extend(input_names)\n",
        "    all_input_names.extend(min_names)\n",
        "    all_input_names.extend(max_names)\n",
        "    quantized_concat_node = create_node(\"QuantizedConcat\",\n",
        "                                        quantized_concat_name, all_input_names)\n",
        "    set_attr_int(quantized_concat_node, \"N\", len(original_inputs))\n",
        "    set_attr_dtype(quantized_concat_node, \"T\", dtypes.quint8)\n",
        "    self.add_output_graph_node(quantized_concat_node)\n",
        "    self.add_dequantize_result_node(quantized_concat_name, original_node.name)\n",
        "\n",
        "  def eightbitize_placeholder_node(self, current_node):\n",
        "    \"\"\"Replaces a placeholder node with a quint8 placeholder node+dequantize.\"\"\"\n",
        "    name = current_node.name\n",
        "\n",
        "    # Convert the placeholder into a quantized type.\n",
        "    output_node = node_def_pb2.NodeDef()\n",
        "    output_node.CopyFrom(current_node)\n",
        "    set_attr_dtype(output_node, \"dtype\", dtypes.quint8)\n",
        "    output_node.name += \"_original_input\"\n",
        "    self.add_output_graph_node(output_node)\n",
        "\n",
        "    # Add a dequantize to convert back to float.\n",
        "    dequantize_node = create_node(\"Dequantize\", name, [\n",
        "        output_node.name, \"quantized_input_min_value\",\n",
        "        \"quantized_input_max_value\"\n",
        "    ])\n",
        "    set_attr_dtype(dequantize_node, \"T\", dtypes.quint8)\n",
        "    set_attr_string(dequantize_node, \"mode\", b\"MIN_FIRST\")\n",
        "    self.add_output_graph_node(dequantize_node)\n",
        "\n",
        "    # For the descent over the graph to work, the dequantize node must be named\n",
        "    # current_node.name.  However, for the feeding of the graph to work, the\n",
        "    # placeholder must have the name current_node.name; so record a final set\n",
        "    # of renames to apply after all processing has been done.\n",
        "    self.final_node_renames[output_node.name] = name\n",
        "    self.final_node_renames[dequantize_node.name] = name + \"_dequantize\"\n",
        "\n",
        "  def eightbitize_reshape_node(self, original_node):\n",
        "    \"\"\"Replaces a Reshape node with the eight bit equivalent sub-graph.\n",
        "    Args:\n",
        "      original_node: Float node to be converted.\n",
        "    Returns:\n",
        "      Subgraph representing the quantized version of the original node.\n",
        "    \"\"\"\n",
        "    namespace_prefix = original_node.name + \"_eightbit\"\n",
        "    quantized_reshape_name = namespace_prefix + \"_quantized_reshape\"\n",
        "    reshape_dims_name, reduction_dims_name = self.add_common_quantization_nodes(\n",
        "        namespace_prefix)\n",
        "    shape_input_name = original_node.input[1]\n",
        "    quantize_input_name, min_input_name, max_input_name = (\n",
        "        self.eightbitize_input_to_node(namespace_prefix, original_node.input[0],\n",
        "                                       reshape_dims_name, reduction_dims_name))\n",
        "    quantized_reshape_node = create_node(\n",
        "        \"QuantizedReshape\", quantized_reshape_name,\n",
        "        [quantize_input_name, shape_input_name, min_input_name, max_input_name])\n",
        "    set_attr_dtype(quantized_reshape_node, \"T\", dtypes.quint8)\n",
        "    self.add_output_graph_node(quantized_reshape_node)\n",
        "    self.add_dequantize_result_node(quantized_reshape_name, original_node.name)\n",
        "\n",
        "  def eightbitize_batch_norm_node(self, original_node):\n",
        "    \"\"\"Replaces a MatMul node with the eight bit equivalent sub-graph.\"\"\"\n",
        "    namespace_prefix = original_node.name + \"_eightbit\"\n",
        "    original_input_name = original_node.input[0]\n",
        "    original_mean_name = original_node.input[1]\n",
        "    original_variance_name = original_node.input[2]\n",
        "    original_beta_name = original_node.input[3]\n",
        "    original_gamma_name = original_node.input[4]\n",
        "    quantized_batch_norm_name = namespace_prefix + \"_quantized_batch_norm\"\n",
        "\n",
        "    reshape_dims_name, reduction_dims_name = self.add_common_quantization_nodes(\n",
        "        namespace_prefix)\n",
        "    quantize_input_name, min_input_name, max_input_name = (\n",
        "        self.eightbitize_input_to_node(namespace_prefix, original_input_name,\n",
        "                                       reshape_dims_name, reduction_dims_name))\n",
        "    quantize_mean_name, min_mean_name, max_mean_name = (\n",
        "        self.eightbitize_input_to_node(namespace_prefix, original_mean_name,\n",
        "                                       reshape_dims_name, reduction_dims_name))\n",
        "    quantize_variance_name, min_variance_name, max_variance_name = (\n",
        "        self.eightbitize_input_to_node(namespace_prefix, original_variance_name,\n",
        "                                       reshape_dims_name, reduction_dims_name))\n",
        "    quantize_beta_name, min_beta_name, max_beta_name = (\n",
        "        self.eightbitize_input_to_node(namespace_prefix, original_beta_name,\n",
        "                                       reshape_dims_name, reduction_dims_name))\n",
        "    quantize_gamma_name, min_gamma_name, max_gamma_name = (\n",
        "        self.eightbitize_input_to_node(namespace_prefix, original_gamma_name,\n",
        "                                       reshape_dims_name, reduction_dims_name))\n",
        "    quantized_batch_norm_node = create_node(\n",
        "        \"QuantizedBatchNormWithGlobalNormalization\", quantized_batch_norm_name,\n",
        "        [\n",
        "            quantize_input_name, min_input_name, max_input_name,\n",
        "            quantize_mean_name, min_mean_name, max_mean_name,\n",
        "            quantize_variance_name, min_variance_name, max_variance_name,\n",
        "            quantize_beta_name, min_beta_name, max_beta_name,\n",
        "            quantize_gamma_name, min_gamma_name, max_gamma_name\n",
        "        ])\n",
        "    set_attr_dtype(quantized_batch_norm_node, \"Tinput\", dtypes.quint8)\n",
        "    set_attr_dtype(quantized_batch_norm_node, \"out_type\", dtypes.qint32)\n",
        "    copy_attr(quantized_batch_norm_node, \"scale_after_normalization\",\n",
        "              original_node.attr[\"scale_after_normalization\"])\n",
        "    copy_attr(quantized_batch_norm_node, \"variance_epsilon\",\n",
        "              original_node.attr[\"variance_epsilon\"])\n",
        "    self.add_output_graph_node(quantized_batch_norm_node)\n",
        "    quantize_down_name = self.add_quantize_down_nodes(original_node,\n",
        "                                                      quantized_batch_norm_name)\n",
        "    self.add_dequantize_result_node(quantize_down_name, original_node.name)\n",
        "\n",
        "  def add_output_graph_node(self, output_node):\n",
        "    \"\"\"Inserts one node into the new graph.\"\"\"\n",
        "    self.output_graph.node.extend([output_node])\n",
        "\n",
        "  def remove_redundant_quantization(self, old_graph):\n",
        "    \"\"\"Removes unneeded pairs of quantize/dequantize ops from the graph.\n",
        "    This is a bit of a tricky function, because it's attempting to spot the\n",
        "    pattern of dequantizing from eight-bit up to float, and then immediately\n",
        "    quantizing back down to eight bits again, that's introduced by previous\n",
        "    passes that do 'key-hole' conversions of individual nodes but have to\n",
        "    convert back to float to match the previous output interface, since they\n",
        "    don't know that the next op can handle quantized tensors.\n",
        "    It works by:\n",
        "     - Looking for Quantize nodes.\n",
        "     - Checking to see if their first input is a Dequantize node.\n",
        "     - Seeing if their min/max inputs come from Min/Max nodes.\n",
        "     - Making sure those Min/Max nodes are being fed from the same Dequantize.\n",
        "     - Or that the Min is indirectly being fed from the same Dequantize as Max.\n",
        "     - Making sure the Dequantize is going through a Reshape (which we add\n",
        "       during the previous pass when we create the quantize sub-graph).\n",
        "     - Looking for the dims Const op for the Min/Max dims.\n",
        "    If all of these conditions are met, then it's a sub-graph pattern that\n",
        "    we know how to optimize out (and is likely the common one we've introduced).\n",
        "    We then rewire the graph to skip it entirely, and then rely on the dead node\n",
        "    removal pass to get rid of any nodes that are no longer needed.\n",
        "    Args:\n",
        "      old_graph: The model we'll be stripping redundant nodes from.\n",
        "    Returns:\n",
        "      A graph with the unnecessary nodes removed.\n",
        "    Raises:\n",
        "      ValueError: Two nodes with the same name were found in the graph.\n",
        "    \"\"\"\n",
        "    old_nodes_map = self.create_nodes_map(old_graph)\n",
        "    self.output_graph = graph_pb2.GraphDef()\n",
        "    inputs_to_rename = {}\n",
        "    # We go through all the nodes, looking for any that match the patterns we\n",
        "    # know how to optimize away.\n",
        "    for node in old_graph.node:\n",
        "      # We always start with a Quantize node, and examine its inputs to see if\n",
        "      # they are in a form that can be removed.\n",
        "      if node.op not in [\"Quantize\", \"QuantizeV2\"]:\n",
        "        continue\n",
        "      dequantize_node_name = node_name_from_input(node.input[0])\n",
        "      if dequantize_node_name not in old_nodes_map:\n",
        "        raise ValueError(\"Input node name '\" + dequantize_node_name +\n",
        "                         \"' not found in node '\" + node.name + \"'\")\n",
        "      dequantize_node = old_nodes_map[dequantize_node_name]\n",
        "      # Do we have a Dequantize feeding in, with the same type as the Quantize?\n",
        "      if dequantize_node.op != \"Dequantize\":\n",
        "        continue\n",
        "      if node.attr[\"T\"] != dequantize_node.attr[\"T\"]:\n",
        "        continue\n",
        "      # Now look at the other inputs, and ensure they're Min/Max nodes.\n",
        "      min_node_name = node_name_from_input(node.input[1])\n",
        "      max_node_name = node_name_from_input(node.input[2])\n",
        "      min_node = old_nodes_map[min_node_name]\n",
        "      max_node = old_nodes_map[max_node_name]\n",
        "      is_min_right_type = (min_node.op in [\"Min\", \"Dequantize\"])\n",
        "      is_max_right_type = (max_node.op in [\"Max\", \"Dequantize\"])\n",
        "      if not is_min_right_type or not is_max_right_type:\n",
        "        print(\"Didn't find expected types on inputs : %s, %s.\" % (min_node.op,\n",
        "                                                                  max_node.op))\n",
        "        continue\n",
        "      min_node_input_name = node_name_from_input(min_node.input[0])\n",
        "      max_node_input_name = node_name_from_input(max_node.input[0])\n",
        "      # There are two different patterns for Min nodes we can recognize, one\n",
        "      # where the input comes directly from the same one as the Max, and\n",
        "      # another where we run it through another Min first, so check for both.\n",
        "      is_same_input = False\n",
        "      if min_node_input_name == max_node_input_name:\n",
        "        is_same_input = True\n",
        "      else:\n",
        "        first_min_node_input = old_nodes_map[min_node_input_name]\n",
        "        if first_min_node_input.op == \"Concat\":\n",
        "          second_min_node_name = node_name_from_input(\n",
        "              first_min_node_input.input[1])\n",
        "          second_min_node = old_nodes_map[second_min_node_name]\n",
        "          if second_min_node.op == \"Min\":\n",
        "            second_min_node_input_name = node_name_from_input(\n",
        "                second_min_node.input[0])\n",
        "            is_same_input = (second_min_node_input_name == max_node_input_name)\n",
        "      if not is_same_input:\n",
        "        print(\"Different min/max inputs: \" + min_node_input_name)\n",
        "        continue\n",
        "      # We recognize this pattern, so mark the graph edges to be rewired to\n",
        "      # route around it entirely, since we know it's a no-op.\n",
        "      dequantize_source_name = node_name_from_input(dequantize_node.input[0])\n",
        "      node_tensor_name = ensure_tensor_name_has_port(node.name)\n",
        "      min_tensor_name = node.name + \":1\"\n",
        "      max_tensor_name = node.name + \":2\"\n",
        "      inputs_to_rename[node_tensor_name] = dequantize_source_name\n",
        "      inputs_to_rename[min_tensor_name] = dequantize_node.input[1]\n",
        "      inputs_to_rename[max_tensor_name] = dequantize_node.input[2]\n",
        "    # Finally we apply all the rewiring we've marked to the graph.\n",
        "    for node in old_graph.node:\n",
        "      for index, input_full_name in enumerate(node.input):\n",
        "        input_name = ensure_tensor_name_has_port(input_full_name)\n",
        "        if input_name in inputs_to_rename:\n",
        "          node.input[index] = inputs_to_rename[input_name]\n",
        "      self.add_output_graph_node(node)\n",
        "    return self.output_graph\n",
        "\n",
        "  def apply_final_node_renames(self):\n",
        "    \"\"\"Applies node renames in self.final_node_renames to self.output_graph.\"\"\"\n",
        "    old_graph = self.output_graph\n",
        "    self.output_graph = graph_pb2.GraphDef()\n",
        "    for node in old_graph.node:\n",
        "      node.name = self.final_node_renames.get(node.name, node.name)\n",
        "      for index, input_name in enumerate(node.input):\n",
        "        node_name = node_name_from_input(input_name)\n",
        "        input_full_name = ensure_tensor_name_has_port(input_name)\n",
        "        if node_name in self.final_node_renames:\n",
        "          node.input[index] = \"%s%s\" % (self.final_node_renames[node_name],\n",
        "                                        input_full_name[len(node_name):])\n",
        "      self.add_output_graph_node(node)\n",
        "    return self.output_graph\n",
        "\n",
        "  def remove_dead_nodes(self, output_names):\n",
        "    \"\"\"Removes nodes that are no longer needed for inference from the graph.\"\"\"\n",
        "    old_output_graph = self.output_graph\n",
        "    self.output_graph = graph_util.extract_sub_graph(old_output_graph,\n",
        "                                                     output_names)\n",
        "\n",
        "  def quantize_weights(self, input_graph, quantization_mode):\n",
        "    \"\"\"Quantize float Const ops.\n",
        "    There are two modes of operations, both replace float Const ops with\n",
        "    quantized values.\n",
        "    1. If quantization_mode is \"weights_rounded\", this function replaces float\n",
        "    Const ops with quantized float Const ops - same as the original op, but\n",
        "    float values being mapped to the center of one of 1<<FLAGS.bitdepth buckets.\n",
        "    This does not change the raw model size, but compression algorithms such as\n",
        "    zip (as used for compressing apks) or bzip2 will achieve a very good\n",
        "    compression ratio.\n",
        "    2. For other quantization modes (\"MIN_COMBINED\" or \"MIN_FIRST\"), float\n",
        "    Const ops are quantized and replaced by a tuple of four ops to perform\n",
        "    the dequantization at runtime:\n",
        "    * eight-bit Const (bucket indices, same shape as original float Const op\n",
        "    * two float Const ops (min and max value of original float Const op)\n",
        "    * Dequantize op to convert the eight-bit consts to float tensors.\n",
        "    The quantization mode is important because we see accuracy problems when\n",
        "    quantizing weights for different situations depending on the algorithm\n",
        "    used. We haven't figured out exactly what the underlying cause is yet,\n",
        "    unfortunately.\n",
        "    Args:\n",
        "      input_graph: A GraphDef of the model containing float Const ops.\n",
        "      quantization_mode: How to quantize and dequantize the values.\n",
        "    Returns:\n",
        "      A GraphDef of the converted graph.\n",
        "    Raises:\n",
        "      ValueError: If quantization_mode is unsupported.\n",
        "    \"\"\"\n",
        "    output_graph = graph_pb2.GraphDef()\n",
        "    for input_node in input_graph.node:\n",
        "      should_quantize = False\n",
        "      if input_node.op == \"Const\":\n",
        "        dtype = dtypes.as_dtype(input_node.attr[\"dtype\"].type)\n",
        "        if dtype == dtypes.float32:\n",
        "          should_quantize = True\n",
        "      if should_quantize:\n",
        "        if quantization_mode == \"weights_rounded\":\n",
        "          output_graph.node.extend(quantize_weight_rounded(input_node))\n",
        "        elif quantization_mode in (b\"MIN_COMBINED\", b\"MIN_FIRST\"):\n",
        "          output_graph.node.extend(\n",
        "              quantize_weight_eightbit(input_node, quantization_mode))\n",
        "        else:\n",
        "          raise ValueError(\"Unsupported quantization mode %s.\" %\n",
        "                           quantization_mode)\n",
        "      else:\n",
        "        output_node = node_def_pb2.NodeDef()\n",
        "        output_node.CopyFrom(input_node)\n",
        "        output_graph.node.extend([output_node])\n",
        "    return output_graph\n",
        "\n",
        "  def set_input_graph(self, new_input_graph):\n",
        "    self.input_graph = new_input_graph\n",
        "    self.nodes_map = self.create_nodes_map(self.input_graph)\n",
        "\n",
        "\n",
        "def main(unused_args):\n",
        "  if not gfile.Exists(FLAGS.input):\n",
        "    print(\"Input graph file '\" + FLAGS.input + \"' does not exist!\")\n",
        "    return -1\n",
        "\n",
        "  known_modes = [\n",
        "      \"round\", \"quantize\", \"eightbit\", \"weights\", \"test\", \"weights_rounded\"\n",
        "  ]\n",
        "  if not any(FLAGS.mode in s for s in known_modes):\n",
        "    print(\"mode is '\" + FLAGS.mode + \"', not in \" + \", \".join(known_modes) +\n",
        "          \".\")\n",
        "    return -1\n",
        "\n",
        "  tf_graph = graph_pb2.GraphDef()\n",
        "  with gfile.Open(FLAGS.input, \"rb\") as f:\n",
        "    data = f.read()\n",
        "    tf_graph.ParseFromString(data)\n",
        "\n",
        "  graph = ops.Graph()\n",
        "  with graph.as_default():\n",
        "    importer.import_graph_def(tf_graph, input_map={}, name=\"\")\n",
        "\n",
        "  quantized_input_range = None\n",
        "  if FLAGS.quantized_input:\n",
        "    quantized_input_range = [\n",
        "        FLAGS.quantized_input_min, FLAGS.quantized_input_max\n",
        "    ]\n",
        "\n",
        "  fallback_quantization_range = None\n",
        "  if (FLAGS.quantized_fallback_min is not None or\n",
        "      FLAGS.quantized_fallback_max is not None):\n",
        "    assert FLAGS.quantized_fallback_min is not None\n",
        "    assert FLAGS.quantized_fallback_max is not None\n",
        "    fallback_quantization_range = [\n",
        "        FLAGS.quantized_fallback_min, FLAGS.quantized_fallback_max\n",
        "    ]\n",
        "\n",
        "  rewriter = GraphRewriter(tf_graph, FLAGS.mode, quantized_input_range,\n",
        "                           fallback_quantization_range)\n",
        "\n",
        "  output_graph = rewriter.rewrite(FLAGS.output_node_names.split(\",\"))\n",
        "\n",
        "  f = gfile.FastGFile(FLAGS.output, \"wb\")\n",
        "  f.write(output_graph.SerializeToString())\n",
        "\n",
        "  return 0\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  app.run()"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}