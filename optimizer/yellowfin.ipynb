{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "from math import ceil, floor\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.training import momentum\n",
        "from tensorflow.python.ops import variable_scope\n",
        "from tensorflow.python.ops import variables\n",
        "from tensorflow.python.framework import ops\n",
        "\n",
        "# Values for gate_gradients.\n",
        "GATE_NONE = 0\n",
        "GATE_OP = 1\n",
        "GATE_GRAPH = 2\n",
        "\n",
        "\n",
        "class YFOptimizer(object):\n",
        "    def __init__(self, lr=0.1, mu=0.0, clip_thresh=None, beta=0.999, curv_win_width=20,\n",
        "                 mu_update_interval=1, zero_debias=True, delta_mu=0.0):\n",
        "        '''\n",
        "        clip thresh is the threshold value on ||lr * gradient||\n",
        "        delta_mu can be place holder/variable/python scalar. They are used for additional\n",
        "        momentum in situations such as asynchronous-parallel training. The default is 0.0\n",
        "        for basic usage of the optimizer.\n",
        "        Args:\n",
        "          lr: python scalar. The initial value of learning rate, we use 1.0 in our paper.\n",
        "          mu: python scalar. The initial value of momentum, we use 0.0 in our paper.\n",
        "          clip_thresh: python scalar. The cliping threshold for tf.clip_by_global_norm.\n",
        "            if None, no clipping will be carried out.\n",
        "          beta: python scalar. The smoothing parameter for estimations.\n",
        "          delta_mu: for extensions. Not necessary in the basic use.\n",
        "        Other features:\n",
        "          If you want to manually control the learning rates, self.lr_factor is\n",
        "          an interface to the outside, it is an multiplier for the internal learning rate\n",
        "          in YellowFin. It is helpful when you want to do additional hand tuning\n",
        "          or some decaying scheme to the tuned learning rate in YellowFin.\n",
        "          Example on using lr_factor can be found here:\n",
        "          https://github.com/JianGoForIt/YellowFin/blob/master/char-rnn-tensorflow/train_YF.py#L140\n",
        "        '''\n",
        "        self._lr = lr\n",
        "        self._mu = mu\n",
        "\n",
        "        self._lr_var = tf.Variable(lr, dtype=tf.float32, name=\"YF_lr\", trainable=False)\n",
        "        self._mu_var = tf.Variable(mu, dtype=tf.float32, name=\"YF_mu\", trainable=False)\n",
        "        # for step scheme or decaying scheme for the learning rates\n",
        "        self.lr_factor = tf.Variable(1.0, dtype=tf.float32, name=\"YF_lr_factor\", trainable=False)\n",
        "        if clip_thresh is not None:\n",
        "            self._clip_thresh_var = tf.Variable(clip_thresh, dtype=tf.float32, name=\"YF_clip_thresh\", trainable=False)\n",
        "        else:\n",
        "            self._clip_thresh_var = None\n",
        "\n",
        "        # the underlying momentum optimizer\n",
        "        self._optimizer = \\\n",
        "            tf.train.MomentumOptimizer(self._lr_var * self.lr_factor, self._mu_var + delta_mu)\n",
        "\n",
        "        # moving average for statistics\n",
        "        self._beta = beta\n",
        "        self._moving_averager = None\n",
        "\n",
        "        # for global step counting\n",
        "        self._global_step = tf.Variable(0, trainable=False)\n",
        "\n",
        "        # for conditional tuning\n",
        "        self._do_tune = tf.greater(self._global_step, tf.constant(0))\n",
        "\n",
        "        self._zero_debias = zero_debias\n",
        "\n",
        "        self._tvars = None\n",
        "\n",
        "        # for curvature range\n",
        "        self._curv_win_width = curv_win_width\n",
        "        self._curv_win = None\n",
        "\n",
        "    def curvature_range(self):\n",
        "        # set up the curvature window\n",
        "        self._curv_win = \\\n",
        "            tf.Variable(np.zeros([self._curv_win_width, ]), dtype=tf.float32, name=\"curv_win\", trainable=False)\n",
        "        self._curv_win = tf.scatter_update(self._curv_win,\n",
        "                                           self._global_step % self._curv_win_width, self._grad_norm_squared)\n",
        "        # note here the iterations start from iteration 0\n",
        "        valid_window = tf.slice(self._curv_win, tf.constant([0, ]),\n",
        "                                tf.expand_dims(tf.minimum(tf.constant(self._curv_win_width), self._global_step + 1),\n",
        "                                               dim=0))\n",
        "        self._h_min_t = tf.reduce_min(valid_window)\n",
        "        self._h_max_t = tf.reduce_max(valid_window)\n",
        "\n",
        "        curv_range_ops = []\n",
        "        with tf.control_dependencies([self._h_min_t, self._h_max_t]):\n",
        "            avg_op = self._moving_averager.apply([self._h_min_t, self._h_max_t])\n",
        "            with tf.control_dependencies([avg_op]):\n",
        "                self._h_min = tf.identity(self._moving_averager.average(self._h_min_t))\n",
        "                self._h_max = tf.identity(self._moving_averager.average(self._h_max_t))\n",
        "        curv_range_ops.append(avg_op)\n",
        "        return curv_range_ops\n",
        "\n",
        "    def grad_variance(self):\n",
        "        grad_var_ops = []\n",
        "        tensor_to_avg = []\n",
        "        for t, g in zip(self._tvars, self._grads):\n",
        "            if isinstance(g, ops.IndexedSlices):\n",
        "                tensor_to_avg.append(\n",
        "                    tf.reshape(tf.unsorted_segment_sum(g.values, g.indices, g.dense_shape[0]), shape=t.get_shape()))\n",
        "            else:\n",
        "                tensor_to_avg.append(g)\n",
        "        avg_op = self._moving_averager.apply(tensor_to_avg)\n",
        "        grad_var_ops.append(avg_op)\n",
        "        with tf.control_dependencies([avg_op]):\n",
        "            self._grad_avg = [self._moving_averager.average(val) for val in tensor_to_avg]\n",
        "            self._grad_avg_squared = [tf.square(val) for val in self._grad_avg]\n",
        "        self._grad_var = self._grad_norm_squared_avg - tf.add_n([tf.reduce_sum(val) for val in self._grad_avg_squared])\n",
        "        return grad_var_ops\n",
        "\n",
        "    def dist_to_opt(self):\n",
        "        dist_to_opt_ops = []\n",
        "        # running average of the norm of gradeint\n",
        "        self._grad_norm = tf.sqrt(self._grad_norm_squared)\n",
        "        avg_op = self._moving_averager.apply([self._grad_norm, ])\n",
        "        dist_to_opt_ops.append(avg_op)\n",
        "        with tf.control_dependencies([avg_op]):\n",
        "            self._grad_norm_avg = self._moving_averager.average(self._grad_norm)\n",
        "            # single iteration distance estimation, note here self._grad_norm_avg is per variable\n",
        "            self._dist_to_opt = self._grad_norm_avg / self._grad_norm_squared_avg\n",
        "        # running average of distance\n",
        "        avg_op = self._moving_averager.apply([self._dist_to_opt])\n",
        "        dist_to_opt_ops.append(avg_op)\n",
        "        with tf.control_dependencies([avg_op]):\n",
        "            self._dist_to_opt_avg = tf.identity(self._moving_averager.average(self._dist_to_opt))\n",
        "        return dist_to_opt_ops\n",
        "\n",
        "    def after_apply(self):\n",
        "        self._moving_averager = tf.train.ExponentialMovingAverage(decay=self._beta, zero_debias=self._zero_debias)\n",
        "        assert self._grads != None and len(self._grads) > 0\n",
        "        after_apply_ops = []\n",
        "\n",
        "        # get per var g**2 and norm**2\n",
        "        self._grad_squared = []\n",
        "        self._grad_norm_squared = []\n",
        "        for v, g in zip(self._tvars, self._grads):\n",
        "            with ops.colocate_with(v):\n",
        "                self._grad_squared.append(tf.square(g))\n",
        "        self._grad_norm_squared = [tf.reduce_sum(grad_squared) for grad_squared in self._grad_squared]\n",
        "\n",
        "        # the following running average on squared norm of gradient is shared by grad_var and dist_to_opt\n",
        "        avg_op = self._moving_averager.apply(self._grad_norm_squared)\n",
        "        with tf.control_dependencies([avg_op]):\n",
        "            self._grad_norm_squared_avg = [self._moving_averager.average(val) for val in self._grad_norm_squared]\n",
        "            self._grad_norm_squared = tf.add_n(self._grad_norm_squared)\n",
        "            self._grad_norm_squared_avg = tf.add_n(self._grad_norm_squared_avg)\n",
        "        after_apply_ops.append(avg_op)\n",
        "\n",
        "        with tf.control_dependencies([avg_op]):\n",
        "            curv_range_ops = self.curvature_range()\n",
        "            after_apply_ops += curv_range_ops\n",
        "            grad_var_ops = self.grad_variance()\n",
        "            after_apply_ops += grad_var_ops\n",
        "            dist_to_opt_ops = self.dist_to_opt()\n",
        "            after_apply_ops += dist_to_opt_ops\n",
        "\n",
        "        return tf.group(*after_apply_ops)\n",
        "\n",
        "    def get_lr_tensor(self):\n",
        "        lr = (1.0 - tf.sqrt(self._mu)) ** 2 / self._h_min\n",
        "        return lr\n",
        "\n",
        "    def get_mu_tensor(self):\n",
        "        const_fact = self._dist_to_opt_avg ** 2 * self._h_min ** 2 / 2 / self._grad_var\n",
        "        coef = tf.Variable([-1.0, 3.0, 0.0, 1.0], dtype=tf.float32, name=\"cubic_solver_coef\")\n",
        "        coef = tf.scatter_update(coef, tf.constant(2), -(3 + const_fact))\n",
        "        roots = tf.py_func(np.roots, [coef], Tout=tf.complex64, stateful=False)\n",
        "\n",
        "        # filter out the correct root\n",
        "        root_idx = tf.logical_and(tf.logical_and(tf.greater(tf.real(roots), tf.constant(0.0)),\n",
        "                                                 tf.less(tf.real(roots), tf.constant(1.0))),\n",
        "                                  tf.less(tf.abs(tf.imag(roots)), 1e-5))\n",
        "        # in case there are two duplicated roots satisfying the above condition\n",
        "        root = tf.reshape(tf.gather(tf.gather(roots, tf.where(root_idx)), tf.constant(0)), shape=[])\n",
        "        tf.assert_equal(tf.size(root), tf.constant(1))\n",
        "\n",
        "        dr = self._h_max / self._h_min\n",
        "        mu = tf.maximum(tf.real(root) ** 2, ((tf.sqrt(dr) - 1) / (tf.sqrt(dr) + 1)) ** 2)\n",
        "        return mu\n",
        "\n",
        "    def update_hyper_param(self):\n",
        "        assign_hyper_ops = []\n",
        "        self._mu = tf.identity(tf.cond(self._do_tune, lambda: self.get_mu_tensor(),\n",
        "                                       lambda: self._mu_var))\n",
        "        with tf.control_dependencies([self._mu]):\n",
        "            self._lr = tf.identity(tf.cond(self._do_tune, lambda: self.get_lr_tensor(),\n",
        "                                           lambda: self._lr_var))\n",
        "\n",
        "        with tf.control_dependencies([self._mu, self._lr]):\n",
        "            self._mu = self._beta * self._mu_var + (1 - self._beta) * self._mu\n",
        "            self._lr = self._beta * self._lr_var + (1 - self._beta) * self._lr\n",
        "            assign_hyper_ops.append(tf.assign(self._mu_var, self._mu))\n",
        "            assign_hyper_ops.append(tf.assign(self._lr_var, self._lr))\n",
        "        assign_hyper_op = tf.group(*assign_hyper_ops)\n",
        "        return assign_hyper_op\n",
        "\n",
        "    def apply_gradients(self, grads_tvars, global_step=None, name=None):\n",
        "        self._grads, self._tvars = zip(*grads_tvars)\n",
        "\n",
        "        with tf.variable_scope(\"apply_updates\"):\n",
        "            if self._clip_thresh_var is not None:\n",
        "                self._grads_clip, self._grads_norm = tf.clip_by_global_norm(self._grads, self._clip_thresh_var)\n",
        "                apply_grad_op = \\\n",
        "                    self._optimizer.apply_gradients(zip(self._grads_clip, self._tvars))\n",
        "            else:\n",
        "                apply_grad_op = \\\n",
        "                    self._optimizer.apply_gradients(zip(self._grads, self._tvars))\n",
        "\n",
        "        with tf.variable_scope(\"after_apply\"):\n",
        "            after_apply_op = self.after_apply()\n",
        "\n",
        "        with tf.variable_scope(\"update_hyper\"):\n",
        "            with tf.control_dependencies([after_apply_op]):\n",
        "                update_hyper_op = self.update_hyper_param()\n",
        "\n",
        "        with tf.control_dependencies([update_hyper_op]):\n",
        "            self._increment_global_step_op = tf.assign(self._global_step, self._global_step + 1)\n",
        "\n",
        "        return tf.group(apply_grad_op, after_apply_op, update_hyper_op, self._increment_global_step_op)\n",
        "\n",
        "    def minimize(self, loss, global_step=None, var_list=None,\n",
        "                 gate_gradients=GATE_OP, aggregation_method=None,\n",
        "                 colocate_gradients_with_ops=False, name=None,\n",
        "                 grad_loss=None):\n",
        "        \"\"\"Adapted from Tensorflow Optimizer base class member function:\n",
        "        Add operations to minimize `loss` by updating `var_list`.\n",
        "        This method simply combines calls `compute_gradients()` and\n",
        "        `apply_gradients()`. If you want to process the gradient before applying\n",
        "        them call `tf.gradients()` and `self.apply_gradients()` explicitly instead\n",
        "        of using this function.\n",
        "        \"\"\"\n",
        "        grads_and_vars = self._optimizer.compute_gradients(\n",
        "            loss, var_list=var_list, gate_gradients=gate_gradients,\n",
        "            aggregation_method=aggregation_method,\n",
        "            colocate_gradients_with_ops=colocate_gradients_with_ops,\n",
        "            grad_loss=grad_loss)\n",
        "\n",
        "        vars_with_grad = [v for g, v in grads_and_vars if g is not None]\n",
        "        if not vars_with_grad:\n",
        "            raise ValueError(\n",
        "                \"No gradients provided for any variable, check your graph for ops\"\n",
        "                \" that do not support gradients, between variables %s and loss %s.\" %\n",
        "                ([str(v) for _, v in grads_and_vars], loss))\n",
        "        for g, v in grads_and_vars:\n",
        "            print(\"g \", g)\n",
        "            print(\"v \", v)\n",
        "\n",
        "        return self.apply_gradients(grads_and_vars)\n",
        "\n",
        "    def compute_gradients(self, loss, var_list, gate_gradients=GATE_OP,\n",
        "                          aggregation_method=None, colocate_gradients_with_ops=False,\n",
        "                          name=None, grad_loss=None):\n",
        "\n",
        "        return self._optimizer.compute_gradients(\n",
        "            loss, var_list=var_list, gate_gradients=gate_gradients,\n",
        "            aggregation_method=aggregation_method,\n",
        "            colocate_gradients_with_ops=colocate_gradients_with_ops,\n",
        "            grad_loss=grad_loss)\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}